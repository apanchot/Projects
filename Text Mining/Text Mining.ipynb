{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # <font color=red>Skip this part</font> Importing the training set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"C:/Users/bl_vi/OneDrive/Documentos/Mestrado - DSBA/201920 - Spring - TM/Project/Corpora/train/\"\n",
    "\n",
    "folders = os.listdir(DATA_PATH)\n",
    "\n",
    "authors_train, books_train, contents_train = [], [], []\n",
    "\n",
    "for folder in folders:\n",
    "    file_names = os.listdir(DATA_PATH + folder)\n",
    "    for file_name in file_names:\n",
    "        with open(DATA_PATH + folder + '/' + file_name, encoding='utf-8', errors = 'ignore', mode = 'r+') as f:\n",
    "            data = f.read()\n",
    "        authors_train.append(folder)\n",
    "        books_train.append(file_name)\n",
    "        contents_train.append(data)\n",
    "\n",
    "df_train = pd.DataFrame(list(zip(authors_train, books_train, contents_train)), columns =['author', 'filename', 'content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # <font color=red>Skip this part</font> Importing the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"C:/Users/bl_vi/OneDrive/Documentos/Mestrado - DSBA/201920 - Spring - TM/Project/Corpora/test/\"\n",
    "\n",
    "folders = os.listdir(DATA_PATH)\n",
    "\n",
    "size_test, books_test, contents_test = [], [], []\n",
    "\n",
    "for folder in folders:\n",
    "    file_names = os.listdir(DATA_PATH + folder)\n",
    "    for file_name in file_names:\n",
    "        with open(DATA_PATH + folder + '/' + file_name, encoding='utf-8', errors = 'ignore', mode = 'r+') as f:\n",
    "            data = f.read()\n",
    "        size_test.append(folder)\n",
    "        books_test.append(file_name)\n",
    "        contents_test.append(data)\n",
    "\n",
    "df_test = pd.DataFrame(list(zip(size_test, books_test, contents_test)), columns =['excerpt_size', 'filename', 'excerpt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # <font color=red>Skip this part</font> Splitting each book in 500-word excerpts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitter(s, n):\n",
    "    pieces = s.split()\n",
    "    excerpt = [\" \".join(pieces[i:i+n]) for i in range(0, len(pieces), n)]\n",
    "    return excerpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.loc[:,'content_split'] = df_train.loc[:,'content'].apply(splitter, n = 500)\n",
    "df_train = df_train.loc[:,['filename','content_split','author']].explode('content_split').reset_index(drop=True)\n",
    "df_train.columns = ['filename', 'excerpt', 'author_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "JoseRodriguesSantos    2316\n",
       "JoseSaramago           2033\n",
       "CamiloCasteloBranco    1549\n",
       "EcaDeQueiros            896\n",
       "AlmadaNegreiros          98\n",
       "LuisaMarquesSilva        90\n",
       "Name: author_name, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.author_name.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trecho = df_train.loc[0,'excerpt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # <font color=red>Skip this part</font> Removing Accented Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_accented_chars(text):\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Title: A Scena do Odio Author: Jose de Almada Negreiros Release Date: September 16, 2007 [EBook #22615] Language: Portuguese A ALVARO DE CAMPOS _Excerptos de um poema desbaratado que foi escripto durante os tres dias e as tres noites que durou a revolucao de 14 de Maio de 1915._ Satanizo-Me Tara na Vara de Moyses! O castigo das serpentes e-Me riso nos dentes, Inferno a arder o Meu cantar! Sou Vermelho-Niagara dos sexos escancarados nos chicotes dos cossacos! Sou Pan-Demonio-Trifauce enfermico de Gula! Sou Genio de Zarathustra em Tacas de Mare-Alta! Sou Raiva de Medusa e Damnacao do Sol! Ladram-Me a Vida por vive-La e so me deram Uma! Hao-de lati-La por sina! agora quero vive-La! Hei-de Poeta canta-La em Gala sonora e dina! Hei-de Gloria desannuvia-La! Hei-de Guindaste ica-La Esfinge da Valla commum onde Me querem rir! Hei-de trovao-clarim leva-La Luz as Almas-Noites do Jardim das Lagrymas! Hei-de bombo rufa-La pompa de Pompeia nos Funeraes de Mim! Hei-de Alfange-Mahoma cantar Sodoma na Voz de Nero! Hei-de ser Fuas sem Virgem do Milagre, hei-de ser galope opiado e doido, opiado e doido..., hei-de ser Attila, hei-de Nero, hei-de Eu, cantar Attila, cantar Nero, cantar Eu! Sou throno de Abandono, mal-fadado, nas iras dos barbaros, meus Avos. Oico ainda da Berlinda d'Eu ser sina gemidos vencidos de fracos, ruidos famintos de saque, ais distantes de Maldicao eterna em Voz antiga! Sou ruinas razas, innocentes como as azas de rapinas afogadas. Sou reliquias de martyres impotentes sequestradas em antros do Vicio e da Virtude. Sou clausura de Sancta professa, Mae exilada do Mal, Hostia d'Angustia no Claustro, freira demente e donzella, virtude sosinha da cella em penitencia do sexo! Sou rasto espesinhado d'Invasores que cruzaram o meu sangue, desvirgando-o. Sou a Raiva atavica dos Tavoras, o sangue bastardo de Nero, o odio do ultimo instante do condemnado innocente! A podenga do Limbo mordeu raivosa as pernas nuas da minh'Alma sem baptismo... Ah! que eu sinto, claramente, que nasci de uma praga de ciumes! Eu sou as sete pragas sobre o Nylo e a Alma dos Borgias a penar! E eu vivo aqui desterrado e Job da Vida-gemea d'Eu ser feliz! E eu vivo aqui sepultado vivo na Verdade de nunca ser Eu! Sou apenas o Mendigo de Mim-Proprio, orphao da Virgem do meu sentir. (Pezam kilos no Meu querer as salas-de-espera de Mim. Tu chegas sempre primeiro... Eu volto sempre amanha... Agora vou esperar que morras. Mas tu es tantos que nao morres... Vou deixar d'esp'rar que morras --Vou deixar d'esp'rar por Mim?!...) Ah! que eu sinto, claramente, que nasci de uma praga de ciumes! Eu sou as sete pragas sobre o Nylo e a Alma dos Borgias a penar! Hei-de, entretanto, gastar a garganta a insultar-te, o besta! Hei-de morder-te a ponta do rabo e por-te as maos no chao, no seu logar! Ahi! Saltimbanco-bando de bandoleiros nefastos! Quadrilheiros contrabandistas da Imbecilidade! Ahi! Espelho-aleijao do Sentimento, macaco-intruja do Alma-realejo! Ahi! maquerelle da Ignorancia! Silenceur do Genio-Tempestade! Spleen da Indigestao! Ahi! meia-tijella,\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_accented_chars(trecho)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # <font color=red>Skip this part</font> Expanding contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTRACTION_MAP = {\n",
    "    \"d'aquela\": \"de aquela\",\n",
    "    \"d'aquella\": \"de aquela\",\n",
    "    \"d'aquellas\": \"de aquelas\",\n",
    "    \"d'aquelle\": \"de aquele\",\n",
    "    \"d'aquelles\": \"de aqueles\",\n",
    "    \"d'aquillo\": \"de aquilo\",\n",
    "    'daquela': \"de aquela\",\n",
    "    'daquelas': \"de aquelas\",\n",
    "    'daquele': \"de aquele\",\n",
    "    'daqueles': \"de aqueles\",\n",
    "    'daquella': \"de aquela\",\n",
    "    'daquelle': \"de aquele\",\n",
    "    'daquelles': \"de aquela\",\n",
    "    'daqueloutro': \"de aquele outro\",\n",
    "    'daquillo': \"de aquilo\",\n",
    "    'daquilo': \"de aquilo\",\n",
    "    \"n'aquella\": \"em aquela\",\n",
    "    \"n'aquellas\": \"em aquelas\",\n",
    "    \"n'aquelle\": \"em aquele\",\n",
    "    \"n'aquelles\": \"em aqueles\",\n",
    "    \"n'aquillo\": \"em aquilo\",\n",
    "    'naquela': \"em aquela\",\n",
    "    'naquelas': \"em aquelas\",\n",
    "    'naquele': \"em aquele\",\n",
    "    'naqueles': \"em aqueles\",\n",
    "    'naquella': \"em aquela\",\n",
    "    'naquelle': \"em aquele\",\n",
    "    'naquilo': \"em aquilo\",\n",
    "    \"d'essa\": \"de essa\",\n",
    "    \"d'essas\": \"de essas\",\n",
    "    \"d'esse\": \"de esse\",\n",
    "    \"d'esses\": \"de essa\",\n",
    "    \"d'esta\": \"de esta\",\n",
    "    \"d'estas\": \"de estas\",\n",
    "    \"d'este\": \"de este\",\n",
    "    \"d'estes\": \"de estes\",\n",
    "    'dessa': \"de essa\",\n",
    "    'dessas': \"de essas\",\n",
    "    'desses': \"de esses\",\n",
    "    'desta': \"de esta\",\n",
    "    'destas': \"de estas\",\n",
    "    'deste': \"de este\",\n",
    "    'destes': \"de estes\",\n",
    "    \"n'essa\": \"em essa\",\n",
    "    \"n'essas\": \"em essas\",\n",
    "    \"n'esse\": \"em esse\",\n",
    "    \"n'esses\": \"em esses\",\n",
    "    \"n'esta\": \"em esta\",\n",
    "    \"n'estas\": \"em estas\",\n",
    "    \"n'este\": \"em este\",\n",
    "    \"n'estes\": \"em estes\",\n",
    "    'nessa': \"em essa\",\n",
    "    'nessas': \"em essas\",\n",
    "    'nesse': \"em esse\",\n",
    "    'nesses': \"em esses\",\n",
    "    'nesta': \"em esta\",\n",
    "    'nestas': \"em estas\",\n",
    "    'neste': \"em este\",\n",
    "    'nestes': \"em estes\",\n",
    "    \"d'isso\": \"de isso\",\n",
    "    \"d'isto\": \"de isto\",\n",
    "    'disso': \"de isso\",\n",
    "    'disto': \"de isto\",\n",
    "    \"n'isso\": \"em isso\",\n",
    "    \"n'isto\": \"em isto\",\n",
    "    'nisso': \"em isso\",\n",
    "    'nisto': \"em isto\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contractions(text, contraction_mapping=CONTRACTION_MAP):\n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), flags=re.IGNORECASE|re.DOTALL)\n",
    "    \n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        expanded_contraction = contraction_mapping.get(match) if contraction_mapping.get(match) else contraction_mapping.get(match.lower())\n",
    "        expanded_contraction = expanded_contraction[0:]\n",
    "        return expanded_contraction\n",
    "    \n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    return expanded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Antes de falar de este ou de aquele outro, devia pensar em aquilo.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expand_contractions(\"Antes de falar deste ou daqueloutro, devia pensar n'aquillo.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # <font color=red>Skip this part</font> Removing Special Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_characters(text, remove_digits=False):\n",
    "    pattern = r'[^a-zA-z0-9\\s\\-]' if not remove_digits else r'[^a-zA-z\\s\\-]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Title A Scena do Odio Author Jose de Almada Negreiros Release Date September 16 2007 [EBook 22615] Language Portuguese A ALVARO DE CAMPOS _Excerptos de um poema desbaratado que foi escripto durante os tres dias e as tres noites que durou a revolucao de 14 de Maio de 1915_ Satanizo-Me Tara na Vara de Moyses O castigo das serpentes e-Me riso nos dentes Inferno a arder o Meu cantar Sou Vermelho-Niagara dos sexos escancarados nos chicotes dos cossacos Sou Pan-Demonio-Trifauce enfermico de Gula Sou Genio de Zarathustra em Tacas de Mare-Alta Sou Raiva de Medusa e Damnacao do Sol Ladram-Me a Vida por vive-La e so me deram Uma Hao-de lati-La por sina agora quero vive-La Hei-de Poeta canta-La em Gala sonora e dina Hei-de Gloria desannuvia-La Hei-de Guindaste ica-La Esfinge da Valla commum onde Me querem rir Hei-de trovao-clarim leva-La Luz as Almas-Noites do Jardim das Lagrymas Hei-de bombo rufa-La pompa de Pompeia nos Funeraes de Mim Hei-de Alfange-Mahoma cantar Sodoma na Voz de Nero Hei-de ser Fuas sem Virgem do Milagre hei-de ser galope opiado e doido opiado e doido hei-de ser Attila hei-de Nero hei-de Eu cantar Attila cantar Nero cantar Eu Sou throno de Abandono mal-fadado nas iras dos barbaros meus Avos Oico ainda da Berlinda dEu ser sina gemidos vencidos de fracos ruidos famintos de saque ais distantes de Maldicao eterna em Voz antiga Sou ruinas razas innocentes como as azas de rapinas afogadas Sou reliquias de martyres impotentes sequestradas em antros do Vicio e da Virtude Sou clausura de Sancta professa Mae exilada do Mal Hostia dAngustia no Claustro freira demente e donzella virtude sosinha da cella em penitencia do sexo Sou rasto espesinhado dInvasores que cruzaram o meu sangue desvirgando-o Sou a Raiva atavica dos Tavoras o sangue bastardo de Nero o odio do ultimo instante do condemnado innocente A podenga do Limbo mordeu raivosa as pernas nuas da minhAlma sem baptismo Ah que eu sinto claramente que nasci de uma praga de ciumes Eu sou as sete pragas sobre o Nylo e a Alma dos Borgias a penar E eu vivo aqui desterrado e Job da Vida-gemea dEu ser feliz E eu vivo aqui sepultado vivo na Verdade de nunca ser Eu Sou apenas o Mendigo de Mim-Proprio orphao da Virgem do meu sentir Pezam kilos no Meu querer as salas-de-espera de Mim Tu chegas sempre primeiro Eu volto sempre amanha Agora vou esperar que morras Mas tu es tantos que nao morres Vou deixar desprar que morras --Vou deixar desprar por Mim Ah que eu sinto claramente que nasci de uma praga de ciumes Eu sou as sete pragas sobre o Nylo e a Alma dos Borgias a penar Hei-de entretanto gastar a garganta a insultar-te o besta Hei-de morder-te a ponta do rabo e por-te as maos no chao no seu logar Ahi Saltimbanco-bando de bandoleiros nefastos Quadrilheiros contrabandistas da Imbecilidade Ahi Espelho-aleijao do Sentimento macaco-intruja do Alma-realejo Ahi maquerelle da Ignorancia Silenceur do Genio-Tempestade Spleen da Indigestao Ahi meia-tijella'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_special_characters(remove_accented_chars(trecho))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # <font color=red>Skip this part</font> Case Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def caseconversion(text, case_type = 'lower'):\n",
    "    if case_type == 'lower':\n",
    "        text = text.lower()\n",
    "    elif case_type == 'upper':\n",
    "        text = text.upper()\n",
    "    elif case_type == 'title':\n",
    "        text = text.title()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"title: a scena do odio author: josé de almada negreiros release date: september 16, 2007 [ebook #22615] language: portuguese a alvaro de campos _excerptos de um poema desbaratado que foi escripto durante os três dias e as três noites que durou a revolução de 14 de maio de 1915._ satanizo-me tara na vara de moysés! o castigo das serpentes é-me riso nos dentes, inferno a arder o meu cantar! sou vermelho-niagára dos sexos escancarados nos chicotes dos cossacos! sou pan-demonio-trifauce enfermiço de gula! sou genio de zarathustra em taças de maré-alta! sou raiva de medusa e damnação do sol! ladram-me a vida por vivê-la e só me deram uma! hão-de lati-la por sina! agora quero vivê-la! hei-de poeta cantá-la em gala sonora e dina! hei-de gloria desannuviá-la! hei-de guindaste içá-la esfinge da valla commum onde me querem rir! hei-de trovão-clarim levá-la luz ás almas-noites do jardim das lagrymas! hei-de bombo rufá-la pompa de pompeia nos funeraes de mim! hei-de alfange-mahoma cantar sodoma na voz de nero! hei-de ser fuas sem virgem do milagre, hei-de ser galope opiado e doido, opiado e doido..., hei-de ser attila, hei-de nero, hei-de eu, cantar attila, cantar nero, cantar eu! sou throno de abandono, mal-fadado, nas iras dos barbaros, meus avós. oiço ainda da berlinda d'eu ser sina gemidos vencidos de fracos, ruidos famintos de saque, ais distantes de maldição eterna em voz antiga! sou ruinas razas, innocentes como as azas de rapinas afogadas. sou reliquias de martyres impotentes sequestradas em antros do vicio e da virtude. sou clausura de sancta professa, mãe exilada do mal, hostia d'angustia no claustro, freira demente e donzella, virtude sosinha da cella em penitencia do sexo! sou rasto espesinhado d'invasores que cruzaram o meu sangue, desvirgando-o. sou a raiva atavica dos tavoras, o sangue bastardo de nero, o odio do ultimo instante do condemnado innocente! a podenga do limbo mordeu raivosa as pernas nuas da minh'alma sem baptismo... ah! que eu sinto, claramente, que nasci de uma praga de ciumes! eu sou as sete pragas sobre o nylo e a alma dos borgias a penar! e eu vivo aqui desterrado e job da vida-gemea d'eu ser feliz! e eu vivo aqui sepultado vivo na verdade de nunca ser eu! sou apenas o mendigo de mim-proprio, orphão da virgem do meu sentir. (pezam kilos no meu querer as salas-de-espera de mim. tu chegas sempre primeiro... eu volto sempre amanhã... agora vou esperar que morras. mas tu és tantos que não morres... vou deixar d'esp'rar que morras --vou deixar d'esp'rar por mim?!...) ah! que eu sinto, claramente, que nasci de uma praga de ciumes! eu sou as sete pragas sobre o nylo e a alma dos borgias a penar! hei-de, entretanto, gastar a garganta a insultar-te, ó bêsta! hei-de morder-te a ponta do rabo e pôr-te as mãos no chão, no seu logar! ahi! saltímbanco-bando de bandoleiros nefastos! quadrilheiros contrabandistas da imbecilidade! ahi! espelho-aleijão do sentimento, macaco-intruja do alma-realejo! ahi! maquerelle da ignorancia! silenceur do genio-tempestade! spleen da indigestão! ahi! meia-tijella,\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caseconversion(trecho)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # <font color=red>Skip this part</font> Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_stemmer(text, stemmer='RSLP'):\n",
    "    ps = RSLPStemmer()\n",
    "    if stemmer=='Snowball':\n",
    "        ps = SnowballStemmer(\"portuguese\")\n",
    "    text = ' '.join([ps.stem(word) for word in text.split()])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pens em irm viaj amanhã'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_stemmer('pensei em irmos viajar amanhã')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pens em irmos viaj amanhã'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_stemmer('pensei em irmos viajar amanhã', stemmer='Snowball')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"title: a scen do odi author: josé de alm negr releas date: septemb 16, 2007 [ebook #22615] language: portugues a alvar de camp _excerpt de um poem desbarat que foi escript dur os trê dia e as trê noit que dur a revoluç de 14 de mai de 1915._ satanizo-m tar na var de moysés! o castig da serp é-m ris no dentes, infern a ard o meu cantar! sou vermelho-niagár do sex escancar no chicot do cossacos! sou pan-demonio-trifauc enfermiç de gula! sou geni de zarathustr em taç de maré-alta! sou raiv de medus e damn do sol! ladram-m a vid por vivê-l e só me der uma! hão-d lati-l por sina! agor quer vivê-la! hei-d poet cantá-l em gal son e dina! hei-d glor desannuviá-la! hei-d guind içá-l esfing da vall commum ond me quer rir! hei-d trovão-clarim levá-l luz ás almas-noit do jardim da lagrymas! hei-d bomb rufá-l pomp de pompe no funera de mim! hei-d alfange-mahom cant sodom na voz de nero! hei-d ser fua sem virgem do milagre, hei-d ser galop opi e doido, opi e doido..., hei-d ser attila, hei-d nero, hei-d eu, cant attila, cant nero, cant eu! sou thron de abandono, mal-fadado, na ira do barbaros, meu avós. oiç aind da berlind d'eu ser sin gem venc de fracos, ruid famint de saque, ai dist de mald etern em voz antiga! sou ruin razas, innoc com as aza de rapin afogadas. sou reliqu de martyr impot sequestr em antr do vici e da virtude. sou claus de sanct professa, mãe exil do mal, host d'angust no claustro, fre dement e donzella, virtud sos da cell em penitenc do sexo! sou rast espesinh d'inva que cruz o meu sangue, desvirgando-o. sou a raiv atav do tavoras, o sang bastard de nero, o odi do ult instant do condemn innocente! a podeng do limb mord raiv as pern nua da minh'alm sem baptismo... ah! que eu sinto, claramente, que nasc de uma prag de ciumes! eu sou as set prag sobr o nyl e a alm do borg a penar! e eu viv aqu desterr e job da vida-geme d'eu ser feliz! e eu viv aqu sepult viv na verdad de nunc ser eu! sou apen o mendig de mim-proprio, orph da virgem do meu sentir. (pez kil no meu quer as salas-de-esp de mim. tu cheg sempr primeiro... eu volt sempr amanhã... agor vou esper que morras. mas tu és tant que não morres... vou deix d'esp'r que morr --v deix d'esp'r por mim?!...) ah! que eu sinto, claramente, que nasc de uma prag de ciumes! eu sou as set prag sobr o nyl e a alm do borg a penar! hei-de, entretanto, gast a gargant a insultar-te, ó bêsta! hei-d morder-t a pont do rab e pôr-t as mão no chão, no seu logar! ahi! saltímbanco-b de bandol nefastos! quadrilh contraband da imbecilidade! ahi! espelho-aleij do sentimento, macaco-intruj do alma-realejo! ahi! maquerell da ignorancia! silenceur do genio-tempestade! spleen da indigestão! ahi! meia-tijella,\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_stemmer(trecho)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # <font color=red>Skip this part</font> Lemmatization\n",
    "\n",
    "It does not seem to work well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = pt_core_news_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(text):\n",
    "    text = nlp(text)\n",
    "    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Eu estar pensar em estudar , mas viajar . Meu sonhar ser fazer isso diariamente .'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatize_text('Eu estava pensando em estudar, mas viajarei. Meu sonho é fazer isso diariamente.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Minha casar ser destruir o dia seguinte o o jogar .'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatize_text('Minha casa foi destruída no dia seguinte ao jogo.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Title : A Scena do Odio Author : José de Almada Negreiros Release Date : September 16 , 2007 [ EBook # 22615 ] Language : Portuguese A ALVARO DE CAMPOS _ Excerptos de um poema desbaratar que ser escripto durante o três dia e o três noite que durar o revolução de 14 de Maio de 1915 . _ Satanizo-Me Tara o Vara de Moysés ! O castigar das serpente é-Me riso o dente , Inferno o arder o Meu cantar ! Sou Vermelho-Niagára dos sexo escancarar o chicote dos cossaco ! Sou Pan-Demonio-Trifauce enfermiço de Gula ! Sou Genio de Zarathustra em Taças de Maré-Alta ! Sou Raiva de Medusa e Damnação do Sol ! Ladram-Me o Vida por vivê-La e só me dar Uma ! Hão-de lati-La por sino ! agora querer vivê-La ! Hei-de Poeta cantá-La em Gala sonoro e dino ! Hei-de Gloria desannuviá-La ! Hei-de Guindaste içá-La Esfinge da Valla commum onde Me querer rir ! Hei-de trovão-clarim levá-La Luz ás Almas-Noites do Jardim das Lagrymas ! Hei-de bombo rufá-La pompa de Pompeia o Funeraes de Mim ! Hei-de Alfange-Mahoma cantar Sodoma o Voz de Nero ! Hei-de ser Fuas sem Virgem do Milagre , hei-de ser galopar opiar e doido , opiar e doido ... , hei-de ser Attila , hei-de Nero , hei-de Eu , cantar Attila , cantar Nero , cantar Eu ! Sou throno de Abandono , mal-fadado , o irar dos barbaros , meu Avós . Oiço ainda da Berlinda d'Eu ser sino gemido vencido de fraco , ruidos faminto de sacar , ai distante de Maldição eterno em Voz antigo ! Sou ruinas razas , innocentes comer o azas de rapinar afogar . Sou reliquias de martyres impotente sequestrar em antro do Vicio e da Virtude . Sou clausurar de Sancta professo , Mãe exilar do Mal , Hostia d'Angustia o Claustro , freira dementar e donzella , virtude sosinha da cella em penitenciar do sexo ! Sou rasto espesinhado d'Invasores que cruzar o meu sangue , desvirgando-o . Sou o Raiva atavica dos Tavoras , o sangue bastardo de Nero , o odio do ultimar instante do condemnado innocente ! A podengo do Limbo morder raivoso o perna nu da minh'Alma sem baptismo ... Ah ! que eu sentir , claramente , que nascer de umar praga de ciumar ! Eu ser o sete praga sobrar o Nylo e o Alma dos Borgias o penar ! E eu viver aqui desterrar e Job da Vida-gemea d'Eu ser feliz ! E eu viver aqui sepultar viver o Verdade de nunca ser Eu ! Sou apenas o Mendigo de Mim-Proprio , orphão da Virgem do meu sentir . ( Pezam kilos o Meu querer o salas-de-espera de Mim . Tu chegar sempre primeiro ... Eu voltar sempre amanhã ... Agora ir esperar que morrer . Mas tu ser tanto que não morrer ... Vou deixar d'esp'rar que morrer --Vou deixar d'esp'rar por Mim ? ! ... ) Ah ! que eu sentir , claramente , que nascer de umar praga de ciumar ! Eu ser o sete praga sobrar o Nylo e o Alma dos Borgias o penar ! Hei-de , entretanto , gastar o garganta o insultar-te , ó bêsta ! Hei-de morder-te o pontar do rabo e pôr-te o mão o chão , o seu logar ! Ahi ! Saltímbanco-bando de bandoleiro nefasto ! Quadrilheiros contrabandista da Imbecilidade ! Ahi ! Espelho-aleijão do Sentimento , macaco-intruja do Alma-realejo ! Ahi ! maquerelle da Ignorancia ! Silenceur do Genio-Tempestade ! Spleen da Indigestão ! Ahi ! meia-tijella ,\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatize_text(trecho)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # <font color=red>Skip this part</font> Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_list = nltk.corpus.stopwords.words('portuguese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text, is_lower_case=False):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Title : Scena Odio Author : José Almada Negreiros Release Date : September 16 , 2007 [ EBook # 22615 ] Language : Portuguese ALVARO CAMPOS _Excerptos poema desbaratado escripto durante três dias três noites durou revolução 14 Maio 1915._ Satanizo-Me Tara Vara Moysés ! castigo serpentes é-Me riso dentes , Inferno arder cantar ! Vermelho-Niagára sexos escancarados chicotes cossacos ! Pan-Demonio-Trifauce enfermiço Gula ! Genio Zarathustra Taças Maré-Alta ! Raiva Medusa Damnação Sol ! Ladram-Me Vida vivê-La deram ! Hão-de lati-La sina ! agora quero vivê-La ! Hei-de Poeta cantá-La Gala sonora dina ! Hei-de Gloria desannuviá-La ! Hei-de Guindaste içá-La Esfinge Valla commum onde querem rir ! Hei-de trovão-clarim levá-La Luz ás Almas-Noites Jardim Lagrymas ! Hei-de bombo rufá-La pompa Pompeia Funeraes Mim ! Hei-de Alfange-Mahoma cantar Sodoma Voz Nero ! Hei-de ser Fuas Virgem Milagre , hei-de ser galope opiado doido , opiado doido ... , hei-de ser Attila , hei-de Nero , hei-de , cantar Attila , cantar Nero , cantar ! throno Abandono , mal-fadado , iras barbaros , Avós . Oiço ainda Berlinda d'Eu ser sina gemidos vencidos fracos , ruidos famintos saque , ais distantes Maldição eterna Voz antiga ! ruinas razas , innocentes azas rapinas afogadas . reliquias martyres impotentes sequestradas antros Vicio Virtude . clausura Sancta professa , Mãe exilada Mal , Hostia d'Angustia Claustro , freira demente donzella , virtude sosinha cella penitencia sexo ! rasto espesinhado d'Invasores cruzaram sangue , desvirgando-o . Raiva atavica Tavoras , sangue bastardo Nero , odio ultimo instante condemnado innocente ! podenga Limbo mordeu raivosa pernas nuas minh'Alma baptismo ... Ah ! sinto , claramente , nasci praga ciumes ! sete pragas sobre Nylo Alma Borgias penar ! vivo aqui desterrado Job Vida-gemea d'Eu ser feliz ! vivo aqui sepultado vivo Verdade nunca ser ! apenas Mendigo Mim-Proprio , orphão Virgem sentir . ( Pezam kilos querer salas-de-espera Mim . chegas sempre primeiro ... volto sempre amanhã ... Agora vou esperar morras . és tantos morres ... Vou deixar d'esp'rar morras -- Vou deixar d'esp'rar Mim ? ! ... ) Ah ! sinto , claramente , nasci praga ciumes ! sete pragas sobre Nylo Alma Borgias penar ! Hei-de , entretanto , gastar garganta insultar-te , ó bêsta ! Hei-de morder-te ponta rabo pôr-te mãos chão , logar ! Ahi ! Saltímbanco-bando bandoleiros nefastos ! Quadrilheiros contrabandistas Imbecilidade ! Ahi ! Espelho-aleijão Sentimento , macaco-intruja Alma-realejo ! Ahi ! maquerelle Ignorancia ! Silenceur Genio-Tempestade ! Spleen Indigestão ! Ahi ! meia-tijella ,\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_stopwords(trecho)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # <font color=red>Skip this part</font> Building a Text Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_corpus(corpus, contraction_expansion=True, accented_char_removal=True, \n",
    "                     text_lower_case=True, stem_or_lemma='stem', special_char_removal=True, stopword_removal=True, \n",
    "                     remove_digits=False):\n",
    "    preprocessed_corpus = []\n",
    "    # preprocess each document in the corpus\n",
    "    for doc in tqdm(corpus):\n",
    "        # remove accented characters\n",
    "        if accented_char_removal:\n",
    "            doc = remove_accented_chars(doc)\n",
    "        # expand contractions\n",
    "        if contraction_expansion:\n",
    "            doc = expand_contractions(doc)\n",
    "        # lowercase the text\n",
    "        if text_lower_case:\n",
    "            doc = doc.lower()\n",
    "        # remove extra newlines\n",
    "            doc = re.sub(r'[\\r|\\n|\\r\\n]+', ' ',doc)\n",
    "        # lemmatize or stem text\n",
    "        if stem_or_lemma == 'stem':\n",
    "            doc = simple_stemmer(doc, stemmer='RSLP')\n",
    "        elif stem_or_lemma == 'lemma':\n",
    "            doc = lemmatize_text(doc)\n",
    "#         else doc = doc\n",
    "        # remove special characters and\\or digits\n",
    "        if special_char_removal:\n",
    "            # insert spaces between special characters to isolate them\n",
    "            special_char_pattern = re.compile(r'([{.(-)!}])')\n",
    "            doc = special_char_pattern.sub(\" \\\\1 \", doc)\n",
    "            doc = remove_special_characters(doc, remove_digits=remove_digits)\n",
    "        # remove extra whitespace\n",
    "        doc = re.sub(' +', ' ', doc)\n",
    "        # remove stopwords\n",
    "        if stopword_removal:\n",
    "            doc = remove_stopwords(doc, is_lower_case=text_lower_case)\n",
    "        preprocessed_corpus.append(doc)\n",
    "    return preprocessed_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # <font color=red>Skip this part</font> Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_list.remove('não')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6982/6982 [14:53<00:00,  7.82it/s]\n"
     ]
    }
   ],
   "source": [
    "df_train.loc[:,'complete_clean'] = preprocess_corpus(corpus = df_train.loc[:,'excerpt'], \n",
    "                                                     stem_or_lemma='lemma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6982/6982 [16:58<00:00,  6.86it/s] \n"
     ]
    }
   ],
   "source": [
    "df_train.loc[:,'medium_clean'] = preprocess_corpus(corpus = df_train.loc[:,'excerpt'],\n",
    "                                                   stopword_removal=False,\n",
    "                                                   stem_or_lemma='lemma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6982/6982 [00:01<00:00, 4121.48it/s]\n"
     ]
    }
   ],
   "source": [
    "df_train.loc[:,'simple_clean'] = preprocess_corpus(corpus= df_train.loc[:,'excerpt'], \n",
    "                                                   contraction_expansion=False, \n",
    "                                                   accented_char_removal=False, \n",
    "                                                   text_lower_case=True, \n",
    "                                                   stem_or_lemma='nothing', \n",
    "                                                   special_char_removal=False,\n",
    "                                                   stopword_removal=False,\n",
    "                                                   remove_digits=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.loc[:,['filename','excerpt','complete_clean','medium_clean','simple_clean','author_name']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_pickle(\"./train_500.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:02<00:00,  4.40it/s]\n"
     ]
    }
   ],
   "source": [
    "df_test.loc[:,'complete_clean'] = preprocess_corpus(corpus = df_test.loc[:,'excerpt'], \n",
    "                                                    stem_or_lemma='lemma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:02<00:00,  4.54it/s]\n"
     ]
    }
   ],
   "source": [
    "df_test.loc[:,'medium_clean'] = preprocess_corpus(corpus = df_test.loc[:,'excerpt'],\n",
    "                                                  stopword_removal=False,\n",
    "                                                  stem_or_lemma='lemma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:00<00:00, 2000.62it/s]\n"
     ]
    }
   ],
   "source": [
    "df_test.loc[:,'simple_clean'] = preprocess_corpus(corpus= df_test.loc[:,'excerpt'], \n",
    "                                                  contraction_expansion=False, \n",
    "                                                  accented_char_removal=False, \n",
    "                                                  text_lower_case=True, \n",
    "                                                  stem_or_lemma='nothing', \n",
    "                                                  special_char_removal=False, \n",
    "                                                  stopword_removal=False,\n",
    "                                                  remove_digits=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.to_pickle(\"./test.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "import spacy\n",
    "import pt_core_news_sm\n",
    "import unicodedata\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from tqdm import tqdm\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import RSLPStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.manifold import TSNE\n",
    "from matplotlib import pyplot as plt\n",
    "from gensim.models import word2vec\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Embedding, GRU, GlobalMaxPooling1D, Conv1D, Dropout\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.metrics import Recall,Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Test Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_pickle(\"./train_500.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_pickle(\"./test.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_frequency = df_train.author_name.isin(['JoseRodriguesSantos','JoseSaramago','CamiloCasteloBranco'])\n",
    "medium_frequency = df_train.author_name.isin(['EcaDeQueiros'])\n",
    "df_train_balanced = pd.concat([df_train.loc[high_frequency,:].iloc[::16,:],\n",
    "                               df_train.loc[medium_frequency,:].iloc[::8,:],\n",
    "                               df_train.loc[~(high_frequency | medium_frequency),:]]).sort_index().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "JoseRodriguesSantos    145\n",
       "JoseSaramago           127\n",
       "EcaDeQueiros           112\n",
       "AlmadaNegreiros         98\n",
       "CamiloCasteloBranco     97\n",
       "LuisaMarquesSilva       90\n",
       "Name: author_name, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_balanced.author_name.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(np.array(df_train_balanced.loc[:,'complete_clean']),\n",
    "                                                    np.array(df_train_balanced.loc[:,'author_name']),\n",
    "                                                    test_size=0.20,\n",
    "                                                    random_state=42,\n",
    "                                                    stratify=df_train_balanced.loc[:,'author_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target Label</th>\n",
       "      <th>Train Count</th>\n",
       "      <th>Test Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>JoseRodriguesSantos</td>\n",
       "      <td>116</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>JoseSaramago</td>\n",
       "      <td>101</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>EcaDeQueiros</td>\n",
       "      <td>90</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>AlmadaNegreiros</td>\n",
       "      <td>78</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>CamiloCasteloBranco</td>\n",
       "      <td>78</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>LuisaMarquesSilva</td>\n",
       "      <td>72</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Target Label  Train Count  Test Count\n",
       "2  JoseRodriguesSantos          116          29\n",
       "4         JoseSaramago          101          26\n",
       "0         EcaDeQueiros           90          22\n",
       "1      AlmadaNegreiros           78          20\n",
       "3  CamiloCasteloBranco           78          19\n",
       "5    LuisaMarquesSilva           72          18"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dict = dict(Counter(y_train))\n",
    "test_dict = dict(Counter(y_test))\n",
    "\n",
    "(pd.DataFrame([[key, train_dict[key], test_dict[key]] for key in train_dict], \n",
    "              columns=['Target Label', 'Train Count', 'Test Count']).sort_values(by=['Train Count', 'Test Count'], \n",
    "                                                                                 ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_classifier(text):\n",
    "    '''This classifier predicts the most frequent class in the training data.'''\n",
    "    return Counter(y_train).most_common(1)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_pred = np.array([baseline_classifier(i) for i in X_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21641791044776118"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy_score(y_test, baseline_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AlmadaNegreiros</th>\n",
       "      <th>CamiloCasteloBranco</th>\n",
       "      <th>EcaDeQueiros</th>\n",
       "      <th>JoseRodriguesSantos</th>\n",
       "      <th>JoseSaramago</th>\n",
       "      <th>LuisaMarquesSilva</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>AlmadaNegreiros</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>CamiloCasteloBranco</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>EcaDeQueiros</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>JoseRodriguesSantos</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>JoseSaramago</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>LuisaMarquesSilva</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     AlmadaNegreiros  CamiloCasteloBranco  EcaDeQueiros  \\\n",
       "AlmadaNegreiros                    0                    0             0   \n",
       "CamiloCasteloBranco                0                    0             0   \n",
       "EcaDeQueiros                       0                    0             0   \n",
       "JoseRodriguesSantos                0                    0             0   \n",
       "JoseSaramago                       0                    0             0   \n",
       "LuisaMarquesSilva                  0                    0             0   \n",
       "\n",
       "                     JoseRodriguesSantos  JoseSaramago  LuisaMarquesSilva  \n",
       "AlmadaNegreiros                       20             0                  0  \n",
       "CamiloCasteloBranco                   19             0                  0  \n",
       "EcaDeQueiros                          22             0                  0  \n",
       "JoseRodriguesSantos                   29             0                  0  \n",
       "JoseSaramago                          26             0                  0  \n",
       "LuisaMarquesSilva                     18             0                  0  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm = metrics.confusion_matrix(y_test, baseline_pred)\n",
    "pd.DataFrame(cm, index=df_train.loc[:,'author_name'].unique(), columns=df_train.loc[:,'author_name'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     precision    recall  f1-score   support\n",
      "\n",
      "    AlmadaNegreiros      0.000     0.000     0.000        20\n",
      "CamiloCasteloBranco      0.000     0.000     0.000        19\n",
      "       EcaDeQueiros      0.000     0.000     0.000        22\n",
      "JoseRodriguesSantos      0.216     1.000     0.356        29\n",
      "       JoseSaramago      0.000     0.000     0.000        26\n",
      "  LuisaMarquesSilva      0.000     0.000     0.000        18\n",
      "\n",
      "           accuracy                          0.216       134\n",
      "          macro avg      0.036     0.167     0.059       134\n",
      "       weighted avg      0.047     0.216     0.077       134\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_test, baseline_pred, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GridSearch\n",
    "1. Bag-of-words and TF-IDF\n",
    "2. Unigrams and Bi-grams\n",
    "3. Naive Bayes, Logistic Regression, SVM, SVM with Stochastic Gradient Descent, Random Forest and Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([('feature', CountVectorizer()),\n",
    "                 ('classifier', MultinomialNB())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = [{'feature': [CountVectorizer()],\n",
    "                 'feature__ngram_range': [(1, 1), (1, 2)],\n",
    "                 'classifier': [MultinomialNB()],\n",
    "                 'classifier__alpha': [1e-5, 1e-4, 1e-2, 1e-1, 1]},\n",
    "                {'feature': [CountVectorizer()],\n",
    "                 'feature__ngram_range': [(1, 1), (1, 2)],\n",
    "                 'classifier': [LogisticRegression(penalty='l2', max_iter=100, random_state=42)],\n",
    "                 'classifier__C': [1, 5, 10]},\n",
    "                {'feature': [CountVectorizer()],\n",
    "                 'feature__ngram_range': [(1, 1), (1, 2)],\n",
    "                 'classifier': [LinearSVC(penalty='l2', random_state=42)],\n",
    "                 'classifier__C': [0.01, 0.1, 1, 5]},\n",
    "                {'feature': [CountVectorizer()],\n",
    "                 'feature__ngram_range': [(1, 1), (1, 2)],\n",
    "                 'classifier': [SGDClassifier(loss='hinge', penalty='l2', max_iter=5, random_state=42)]},\n",
    "                {'feature': [CountVectorizer()],\n",
    "                 'feature__ngram_range': [(1, 1), (1, 2)],\n",
    "                 'classifier': [RandomForestClassifier(n_estimators=10, random_state=42)]},\n",
    "                {'feature': [CountVectorizer()],\n",
    "                 'feature__ngram_range': [(1, 1), (1, 2)],\n",
    "                 'classifier': [GradientBoostingClassifier(n_estimators=10, random_state=42)]},\n",
    "                {'feature': [TfidfVectorizer()],\n",
    "                 'feature__ngram_range': [(1, 1), (1, 2)],\n",
    "                 'classifier': [MultinomialNB()],\n",
    "                 'classifier__alpha': [1e-5, 1e-4, 1e-2, 1e-1, 1]},\n",
    "                {'feature': [TfidfVectorizer()],\n",
    "                 'feature__ngram_range': [(1, 1), (1, 2)],\n",
    "                 'classifier': [LogisticRegression(penalty='l2', max_iter=100, random_state=42)],\n",
    "                 'classifier__C': [1, 5, 10]},\n",
    "                {'feature': [TfidfVectorizer()],\n",
    "                 'feature__ngram_range': [(1, 1), (1, 2)],\n",
    "                 'classifier': [LinearSVC(penalty='l2', random_state=42)],\n",
    "                 'classifier__C': [0.01, 0.1, 1, 5]},\n",
    "                {'feature': [TfidfVectorizer()],\n",
    "                 'feature__ngram_range': [(1, 1), (1, 2)],\n",
    "                 'classifier': [SGDClassifier(loss='hinge', penalty='l2', max_iter=5, random_state=42)]},\n",
    "                {'feature': [TfidfVectorizer()],\n",
    "                 'feature__ngram_range': [(1, 1), (1, 2)],\n",
    "                 'classifier': [RandomForestClassifier(n_estimators=10, random_state=42)]},\n",
    "                {'feature': [TfidfVectorizer()],\n",
    "                 'feature__ngram_range': [(1, 1), (1, 2)],\n",
    "                 'classifier': [GradientBoostingClassifier(n_estimators=10, random_state=42)]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GridSearchCV(pipe, search_space, cv=5, verbose=2, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    3.3s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:  3.6min finished\n"
     ]
    }
   ],
   "source": [
    "clf = clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_estimator_.get_params()[\"classifier\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
       "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
       "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, use_idf=True, vocabulary=None)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_estimator_.get_params()[\"feature\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AlmadaNegreiros</th>\n",
       "      <th>CamiloCasteloBranco</th>\n",
       "      <th>EcaDeQueiros</th>\n",
       "      <th>JoseRodriguesSantos</th>\n",
       "      <th>JoseSaramago</th>\n",
       "      <th>LuisaMarquesSilva</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>AlmadaNegreiros</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>CamiloCasteloBranco</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>EcaDeQueiros</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>JoseRodriguesSantos</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>JoseSaramago</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>LuisaMarquesSilva</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     AlmadaNegreiros  CamiloCasteloBranco  EcaDeQueiros  \\\n",
       "AlmadaNegreiros                   20                    0             0   \n",
       "CamiloCasteloBranco                0                   19             0   \n",
       "EcaDeQueiros                       0                    0            21   \n",
       "JoseRodriguesSantos                0                    0             0   \n",
       "JoseSaramago                       0                    0             0   \n",
       "LuisaMarquesSilva                  0                    0             0   \n",
       "\n",
       "                     JoseRodriguesSantos  JoseSaramago  LuisaMarquesSilva  \n",
       "AlmadaNegreiros                        0             0                  0  \n",
       "CamiloCasteloBranco                    0             0                  0  \n",
       "EcaDeQueiros                           0             1                  0  \n",
       "JoseRodriguesSantos                   29             0                  0  \n",
       "JoseSaramago                           0            26                  0  \n",
       "LuisaMarquesSilva                      0             0                 18  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm = metrics.confusion_matrix(y_test, prediction)\n",
    "pd.DataFrame(cm, index=df_train.loc[:,'author_name'].unique(), columns=df_train.loc[:,'author_name'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     precision    recall  f1-score   support\n",
      "\n",
      "    AlmadaNegreiros      1.000     1.000     1.000        20\n",
      "CamiloCasteloBranco      1.000     1.000     1.000        19\n",
      "       EcaDeQueiros      1.000     0.955     0.977        22\n",
      "JoseRodriguesSantos      1.000     1.000     1.000        29\n",
      "       JoseSaramago      0.963     1.000     0.981        26\n",
      "  LuisaMarquesSilva      1.000     1.000     1.000        18\n",
      "\n",
      "           accuracy                          0.993       134\n",
      "          macro avg      0.994     0.992     0.993       134\n",
      "       weighted avg      0.993     0.993     0.993       134\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_test, prediction, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['JoseSaramago', 'AlmadaNegreiros', 'LuisaMarquesSilva',\n",
       "       'EcaDeQueiros', 'CamiloCasteloBranco', 'JoseRodriguesSantos',\n",
       "       'JoseSaramago', 'AlmadaNegreiros', 'LuisaMarquesSilva',\n",
       "       'EcaDeQueiros', 'CamiloCasteloBranco', 'JoseRodriguesSantos'],\n",
       "      dtype='<U19')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(np.array(df_test.loc[:,'complete_clean']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we are using the training set with samples from which were not removed stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(np.array(df_train_balanced.loc[:,'medium_clean']),\n",
    "                                                    np.array(df_train_balanced.loc[:,'author_name']),\n",
    "                                                    test_size=0.20,\n",
    "                                                    random_state=42,\n",
    "                                                    stratify=df_train_balanced.loc[:,'author_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([('feature', CountVectorizer()),\n",
    "                 ('classifier', MultinomialNB())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = [{'feature': [CountVectorizer()],\n",
    "                 'feature__ngram_range': [(1, 1), (1, 2)],\n",
    "                 'classifier': [MultinomialNB()],\n",
    "                 'classifier__alpha': [1e-5, 1e-4, 1e-2, 1e-1, 1]},\n",
    "                {'feature': [CountVectorizer()],\n",
    "                 'feature__ngram_range': [(1, 1), (1, 2)],\n",
    "                 'classifier': [LogisticRegression(penalty='l2', max_iter=100, random_state=42)],\n",
    "                 'classifier__C': [1, 5, 10]},\n",
    "                {'feature': [CountVectorizer()],\n",
    "                 'feature__ngram_range': [(1, 1), (1, 2)],\n",
    "                 'classifier': [LinearSVC(penalty='l2', random_state=42)],\n",
    "                 'classifier__C': [0.01, 0.1, 1, 5]},\n",
    "                {'feature': [CountVectorizer()],\n",
    "                 'feature__ngram_range': [(1, 1), (1, 2)],\n",
    "                 'classifier': [SGDClassifier(loss='hinge', penalty='l2', max_iter=5, random_state=42)]},\n",
    "                {'feature': [CountVectorizer()],\n",
    "                 'feature__ngram_range': [(1, 1), (1, 2)],\n",
    "                 'classifier': [RandomForestClassifier(n_estimators=10, random_state=42)]},\n",
    "                {'feature': [CountVectorizer()],\n",
    "                 'feature__ngram_range': [(1, 1), (1, 2)],\n",
    "                 'classifier': [GradientBoostingClassifier(n_estimators=10, random_state=42)]},\n",
    "                {'feature': [TfidfVectorizer()],\n",
    "                 'feature__ngram_range': [(1, 1), (1, 2)],\n",
    "                 'classifier': [MultinomialNB()],\n",
    "                 'classifier__alpha': [1e-5, 1e-4, 1e-2, 1e-1, 1]},\n",
    "                {'feature': [TfidfVectorizer()],\n",
    "                 'feature__ngram_range': [(1, 1), (1, 2)],\n",
    "                 'classifier': [LogisticRegression(penalty='l2', max_iter=100, random_state=42)],\n",
    "                 'classifier__C': [1, 5, 10]},\n",
    "                {'feature': [TfidfVectorizer()],\n",
    "                 'feature__ngram_range': [(1, 1), (1, 2)],\n",
    "                 'classifier': [LinearSVC(penalty='l2', random_state=42)],\n",
    "                 'classifier__C': [0.01, 0.1, 1, 5]},\n",
    "                {'feature': [TfidfVectorizer()],\n",
    "                 'feature__ngram_range': [(1, 1), (1, 2)],\n",
    "                 'classifier': [SGDClassifier(loss='hinge', penalty='l2', max_iter=5, random_state=42)]},\n",
    "                {'feature': [TfidfVectorizer()],\n",
    "                 'feature__ngram_range': [(1, 1), (1, 2)],\n",
    "                 'classifier': [RandomForestClassifier(n_estimators=10, random_state=42)]},\n",
    "                {'feature': [TfidfVectorizer()],\n",
    "                 'feature__ngram_range': [(1, 1), (1, 2)],\n",
    "                 'classifier': [GradientBoostingClassifier(n_estimators=10, random_state=42)]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GridSearchCV(pipe, search_space, cv=5, verbose=2, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    2.7s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:  3.9min finished\n"
     ]
    }
   ],
   "source": [
    "clf = clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_estimator_.get_params()[\"classifier\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
       "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_estimator_.get_params()[\"feature\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AlmadaNegreiros</th>\n",
       "      <th>CamiloCasteloBranco</th>\n",
       "      <th>EcaDeQueiros</th>\n",
       "      <th>JoseRodriguesSantos</th>\n",
       "      <th>JoseSaramago</th>\n",
       "      <th>LuisaMarquesSilva</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>AlmadaNegreiros</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>CamiloCasteloBranco</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>EcaDeQueiros</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>JoseRodriguesSantos</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>JoseSaramago</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>LuisaMarquesSilva</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     AlmadaNegreiros  CamiloCasteloBranco  EcaDeQueiros  \\\n",
       "AlmadaNegreiros                   19                    0             0   \n",
       "CamiloCasteloBranco                0                   19             0   \n",
       "EcaDeQueiros                       0                    0            21   \n",
       "JoseRodriguesSantos                0                    0             0   \n",
       "JoseSaramago                       0                    0             0   \n",
       "LuisaMarquesSilva                  0                    0             0   \n",
       "\n",
       "                     JoseRodriguesSantos  JoseSaramago  LuisaMarquesSilva  \n",
       "AlmadaNegreiros                        0             1                  0  \n",
       "CamiloCasteloBranco                    0             0                  0  \n",
       "EcaDeQueiros                           0             1                  0  \n",
       "JoseRodriguesSantos                   28             1                  0  \n",
       "JoseSaramago                           0            26                  0  \n",
       "LuisaMarquesSilva                      0             1                 17  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm = metrics.confusion_matrix(y_test, prediction)\n",
    "pd.DataFrame(cm, index=df_train.loc[:,'author_name'].unique(), columns=df_train.loc[:,'author_name'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     precision    recall  f1-score   support\n",
      "\n",
      "    AlmadaNegreiros      1.000     0.950     0.974        20\n",
      "CamiloCasteloBranco      1.000     1.000     1.000        19\n",
      "       EcaDeQueiros      1.000     0.955     0.977        22\n",
      "JoseRodriguesSantos      1.000     0.966     0.982        29\n",
      "       JoseSaramago      0.867     1.000     0.929        26\n",
      "  LuisaMarquesSilva      1.000     0.944     0.971        18\n",
      "\n",
      "           accuracy                          0.970       134\n",
      "          macro avg      0.978     0.969     0.972       134\n",
      "       weighted avg      0.974     0.970     0.971       134\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_test, prediction, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['JoseSaramago', 'AlmadaNegreiros', 'LuisaMarquesSilva',\n",
       "       'EcaDeQueiros', 'CamiloCasteloBranco', 'JoseRodriguesSantos',\n",
       "       'JoseSaramago', 'AlmadaNegreiros', 'LuisaMarquesSilva',\n",
       "       'EcaDeQueiros', 'CamiloCasteloBranco', 'JoseRodriguesSantos'],\n",
       "      dtype='<U19')"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(np.array(df_test.loc[:,'medium_clean']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word-Embeddings (CBOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_corpus = [nltk.WordPunctTokenizer().tokenize(document) for document in df_train.loc[:,'simple_clean']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = word2vec.Word2Vec(tokenized_corpus, \n",
    "                              size=100,\n",
    "                              window=30, \n",
    "                              min_count=1,\n",
    "                              sample=1e-3, \n",
    "                              iter=50, \n",
    "                              workers=8, \n",
    "                              sg=0) #0:CBOW/1:SKIP-GRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cego': ['escuros', 'pistola', 'cegos', 'óculos', 'estrábico'],\n",
       " 'vermelho': ['linho', 'roxo', 'verde', 'cinzento', 'reluzia'],\n",
       " 'lisboa': ['madrid', 'portugal', 'coimbra', 'paris', 'angola'],\n",
       " 'noite': ['hora', 'madrugada', 'manhã', 'tarde', 'noute']}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_words = {search_term: [item[0] for item in w2v_model.wv.most_similar([search_term], topn=5)]\n",
    "                 for search_term in ['cego', 'vermelho', 'lisboa', 'noite']}\n",
    "similar_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAz0AAAHSCAYAAADG5aULAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nOzdeXRV5aH///cmDDKJtNJetELw/kCUJCQhkjAHUcRev1UECiWWoRfRIkr1qmipqCjX1nIdqFatV4EqIApVtLcD4gBCQUggjBKC9gACKiBQBhGS7N8fJxwJg6KEBA7v11quc/Zz9n72s49nufzkmYIwDJEkSZKkeFWlshsgSZIkSSeSoUeSJElSXDP0SJIkSYprhh5JkiRJcc3QI0mSJCmuGXokSZIkxbWqld2AY3H22WeHiYmJld0MSZIkSSexvLy8LWEYNji0/JQIPYmJieTm5lZ2MyRJkiSdxIIgWHukcoe3SZIkSYprhh5JkiRJcc3QI0mSJCmuGXokSZIkxTVDjyRJkqS4ZuiRJEmSFNcMPZIkSZLimqFHkiRJUlwz9EiSJEmKa4YeSZIkSXHN0PMtRSIRkpKSKrsZilN16tQ5Yvm+ffv44Q9/SJcuXRg2bFgFt0qSJOnUVLWyG3A6KioqompVv3p9c9WrV+cvf/lLZTdDkiTplGJPz3EoLi7muuuuo0WLFnTt2pXPP/+c/Px8srKySElJoXv37mzbtg2A7OxsfvnLX9KpUycee+wxXn/9dTIzM0lLS+PSSy/lk08+qeSn0bcRiURo3rw5gwYNIikpiZycHGbOnEm7du1o2rQpCxYsYMGCBbRt25a0tDTatm1LQUEBAOPHj+eaa66hW7duNG3alDvuuKNM3SNGjKBly5ZkZWXFfh/+biRJkr45Q89xKCws5MYbb2TFihWcddZZTJs2jX79+vGb3/yGpUuXkpyczH333Rc7f/v27cyaNYv/+q//on379syfP5/FixfTp08fHnrooUp8Eh2PNWvWMGzYMJYuXcqqVauYNGkSc+bMYcyYMfz3f/83zZs3Z/bs2SxevJhRo0bxy1/+MnZtfn4+U6ZMYdmyZUyZMoX169cDsHv3brKysliyZAkdO3bkmWeeAfB3I0mS9C04xuo4NGnShNTUVABatWrFBx98wPbt2+nUqRMA/fv3p1evXrHze/fuHXv/0Ucf0bt3bzZt2sS+ffto0qRJxTZe5aZJkyYkJycD0KJFC7p06UIQBCQnJxOJRNixYwf9+/ensLCQIAjYv39/7NouXbpQr149AC666CLWrl3LeeedR/Xq1bnyyiuB6G/rjTfeAPzdSJIkfRv29HwTkyZC80RIqAJd2lNj797YRwkJCWzfvv0rL69du3bs/U033cTQoUNZtmwZTz/9NHsPqksnsYN/A80TYfqr1KhRI/ZxlSpVYsdVqlShqKiIu+++m86dO7N8+XJef/31Mv+uD742ISGBoqIiAKpVq0YQBIeV+7uRJEn65gw9x2rSRLhtMPRYC+NC6LYBPt4QLS9Vr1496tevz7vvvgvA888/H+v1OdSOHTs499xzAZgwYcKJb7+O36G/gR5rYfRdsGPHV1528L/r8ePHH1cT/N1IkiR9c4aeYzVqBAzcAy2IDgpsBpwVRssPMmHCBG6//XZSUlLIz89n5MiRR6zu3nvvpVevXnTo0IGzzz77hDdf5eDQ30ALoNde+PSrFxO44447uOuuu2jXrh3FxcXH1QR/N5IkSd9cEIZhZbfha2VkZIS5ubmV24iEKtG/7h88C6oIGBhAcUlltUoVyd+AJEnSSS0IgrwwDDMOLben51g1bQQFh5QVlJbr9OBvQJIk6ZRk6DlWI0fDuFqwguhf91cQPR45upIbpgrjb0CSJOmU5JLVx6pvTvR11AgoXBf96/6Y0V+WK/75G5AkSTolOadHkiRJUlxwTo+OyciRI5k5cyYAjz76KHv27KnkFkmSJEnHx54eHVViYiK5ubkujSxJkqRTgj09p6lIJMKFF17IddddR4sWLejatSuff/45+fn5ZGVlkZKSQvfu3dm2bRsAAwYMYOrUqYwdO5aNGzfSuXNnOnfuDMCMGTNo06YN6enp9OrVi127dlXmo0mSJEnHxNBzGigsLOTGG29kxYoVnHXWWUybNo1+/frxm9/8hqVLl5KcnMx9991X5pqbb76Zc845h7fffpu3336bLVu28MADDzBz5kwWLVpERkYGDz/8cCU9kSRJknTsXL3tNNCkSRNSU1MBaNWqFR988AHbt2+nU6dOAPTv359evXp9ZR3z589n5cqVtGvXDoB9+/bRpk2bE9twSZIkqRwYeuLRpIlfLquceA419gWxjxISEti+ffs3rjIMQy677DImT55cni2VJEmSTjiHt8WbSRPhtsHQYy2MC6HbBvh4Q7S8VL169ahfvz7vvvsuAM8//3ys1+dgdevWZefOnQBkZWUxd+5c1qxZA8CePXtYvXp1BTyQJEmSdHzs6Yk3o0bAwD3QovS4GXBWGC0/aBPNCRMmcMMNN7Bnzx7OP/98xo0bd1hVgwcP5oorrqBhw4a8/fbbjB8/np/85Cd88cUXADzwwAM0a9asAh5KkiRJ+vZcsjreJFSJ9vAcHGeLgIEBFJdUVqskSZKkE84lq08XTRtBwSFlBaXlkiRJ0mnI0BNvRo6GcbVgBdEenhVEj0eOruSGSZIkSZXDOT3x5sC8nQOrtzVtBGNGl5nPI0mSJJ1ODD3xqG+OIUeSJEkq5fA2SZIkSXHN0CNJkiQprhl6JEmSJMU1Q48kSZKkuGbokSRJkhTXDD3SMcjOziY3N7dC7zl+/HiGDh1aofeUJEmKR4Ye6TgUFxdXdhMkSZL0NQw9iluRSITmzZszaNAgkpKSyMnJYebMmbRr146mTZuyYMECFixYQNu2bUlLS6Nt27YUFBQA8Pnnn9OnTx9SUlLo3bs3n3/+eazeOnXqMHLkSDIzM5k3bx6JiYls2bIFgNzcXLKzswHYvHkzl112Genp6Vx//fU0btw4dt7VV19Nq1ataNGiBX/4wx9idY8bN45mzZrRqVMn5s6dGyt//fXXyczMJC0tjUsvvZRPPvnkRH99kiRJccPQo7i2Zs0ahg0bxtKlS1m1ahWTJk1izpw5jBkzhv/+7/+mefPmzJ49m8WLFzNq1Ch++ctfAvDkk09Sq1Ytli5dyogRI8jLy4vVuXv3bpKSknjvvfdo3779Ue993333cckll7Bo0SK6d+/OunXrYp8999xz5OXlkZuby9ixY9m6dSubNm3innvuYe7cubzxxhusXLkydn779u2ZP38+ixcvpk+fPjz00EMn4NuSJEmKT1UruwHSidSkSROSk5MBaNGiBV26dCEIApKTk4lEIuzYsYP+/ftTWFhIEATs378fgNmzZ3PzzTcDkJKSQkpKSqzOhIQEevTo8bX3njNnDq+88goA3bp1o379+rHPxo4dG/ts/fr1FBYW8vHHH5OdnU2DBg0A6N27N6tXrwbgo48+onfv3mzatIl9+/bRpEmT4/1qJEmSThv29Ci+TJoIzRMhoQp0aU+NvXtjH1WpUoUaNWrE3hcVFXH33XfTuXNnli9fzuuvv87eg84PguCItzjjjDNISEiIHVetWpWSkhKAMteHYXjE69955x1mzpzJvHnzWLJkCWlpabHrjnbPm266iaFDh7Js2TKefvrpMveRJEnSVzP0KH5Mmgi3DYYea2FcCN02wMcbouVHsWPHDs4991wgulraAR07dmTixOh1y5cvZ+nSpUetIzExMTb8bdq0abHy9u3b89JLLwEwY8YMtm3bFrtn/fr1qVWrFqtWrWL+/PkAZGZm8s4777B161b279/Pyy+/fMR2Tpgw4Zi/EkmSJBl6FE9GjYCBe6AF0YGbzYCzwmj5Udxxxx3cddddtGvXrsxKbD//+c/ZtWsXKSkpPPTQQ7Ru3fqoddxzzz0MGzaMDh06lOkBuueee5gxYwbp6en89a9/pWHDhtStW5du3bpRVFRESkoKd999N1lZWQA0bNiQe++9lzZt2nDppZeSnp4eq+vee++lV69edOjQgbPPPvtbf0WSJEmno+BoQ3BOJhkZGWFF75GiU1BClWgPz8Ez1YqAgQEUl1R4c7744gsSEhKoWrUq8+bN4+c//zn5+fkV3g5JkqTTRRAEeWEYZhxa7kIGih9NG0HB2mhPzwEFpeWVYN26dfz4xz+mpKSE6tWr88wzz1RKOyRJkk53hh7Fj5Gjo3N6Bu6BC4gGnnG1YMzoSmlO06ZNWbx4caXcW5IkSV8y9Ch+9M2Jvo4aAYXroj08Y0Z/WS5JkqTTkqFH8aVvjiFHkiRJZbh6myRJkqS4ZuiRJEmSFNcMPZIkSd9QJBIhKSkJiG5uPXTo0EpukaSvYuiRJEmnlTAMKSmp+P3bJFUeQ48kSTolDR8+nN///vex43vvvZf/+Z//4be//S0XX3wxKSkp3HPPPUC0Z+bCCy9kyJAhpKens379eurUqcPw4cNp1aoVl156KQsWLCA7O5vzzz+f1157DYDi4mJuv/32WH1PP/30EduyceNGunXrRtOmTbnjjjti5ZMnTyY5OZmkpCSGDx9+Ar8NSV+l3EJPEAQJQRAsDoLgz6XHTYIgeC8IgsIgCKYEQVC9tLxG6fGa0s8Ty6sNkiTp9NGnTx+mTJkSO37ppZdo0KABhYWFLFiwgPz8fPLy8pg9ezYABQUF9OvXj8WLF9O4cWN2795NdnY2eXl51K1bl1/96le88cYbvPLKK4wcORKAZ599lnr16rFw4UIWLlzIM888wz//+c/D2pKfn8+UKVNYtmwZU6ZMYf369WzcuJHhw4fz1ltvkZ+fz8KFC3n11Vcr5suRVEZ5Llk9DHgfOLP0+DfAI2EYvhgEwVPAfwJPlr5uC8Pw/wuCoE/peb3LsR2SJOk0kJaWxqeffsrGjRvZvHkz9evXZ+nSpcyYMYO0tDQAdu3aRWFhIY0aNaJx48ZkZWXFrq9evTrdunUDIDk5mRo1alCtWjWSk5OJRCIAzJgxg6VLlzJ16lQAduzYQWFhIc2aNSvTli5dulCvXj0ALrroItauXcvWrVvJzs6mQYMGAOTk5DB79myuvvrqE/q9SDpcuYSeIAh+APwHMBq4NQiCALgE6Ft6ygTgXqKh56rS9wBTgceDIAjCMAzLoy2SJCmOTZpYZhPqnhemMnXqVD7++GP69OlDJBLhrrvu4vrrry9zWSQSoXbt2mXKqlWrRvR/WaBKlSrUqFEj9r6oqAiIzv/53e9+x+WXX35YfQc7cC1AQkICRUVF+L820smjvIa3PQrcARyYFfhdYHsYhkWlxx8B55a+PxdYD1D6+Y7S8yVJko5u0kS4bTD0WAvjQuixlj5z/s6Ljz/O1KlT6dmzJ5dffjnPPfccu3btAmDDhg18+umn3/qWl19+OU8++ST79+8HYPXq1ezevfuYrs3MzGTWrFls2bKF4uJiJk+eTKdOnb51WyR9e8fd0xMEwZXAp2EY5gVBkH2g+Ainhsfw2cH1DgYGAzRq1Oh4mylJkk51o0bAwD3QovS4BbQYvJedYyKc27YdDRs2pGHDhrz//vu0adMGgDp16vDCCy+QkJDwrW45aNAgIpEI6enphGFIgwYNjnleTsOGDXnwwQfp3LkzYRjywx/+kKuuuupbtUPS8QmOt+s1CIIHgZ8CRcAZROf0vAJcDvxbGIZFQRC0Ae4Nw/DyIAj+Xvp+XhAEVYGPgQZfNbwtIyMjzM3NPa52SpKkU1xClWgPz8F/si0CBgZQ7BLUkiAIgrwwDDMOLT/u4W1hGN4VhuEPwjBMBPoAb4VhmAO8DfQsPa0/ML30/Wulx5R+/pbzeSRJ0tdq2ggKDikrKC2XpK9wIvfpGU50UYM1ROfsPFta/izw3dLyW4E7T2AbJElSvBg5GsbVghVEe3hWED0eObqSGybpZFeeS1YThuE7wDul7z8EWh/hnL1Ar/K8ryRJOg30zYm+HrR6G2NGf1kuSUdRrqFHkiTphOqbY8iR9I2dyOFtkiRJklTpDD2SJEmS4pqhR5IkSVJcM/RIkiRJimuGHkmSJElxzdAjSZIkKa4ZeiRJkiTFNUOPJEmSpLhm6JEkSZIU1ww9kiRJkuKaoUeSJElSXDP0SJIkSYprhh5JkiRJcc3QI0mSJCmuGXokSZIkxTVDjyRJkqS4ZujRaW379u38/ve/P646xo8fz9ChQ8upRZIkSSpvhh6d1r5p6AnDkJKSkhPYIkmSJJU3Q49Oa3feeScffPABqamp3HLLLXTp0oX09HSSk5OZPn06AJFIhAsvvJAhQ4aQnp7O+vXrGTduHM2aNaNTp07MnTs3Vt/mzZvp0aMHF198MRdffHGZzyRJklQ5gjAMK7sNXysjIyPMzc2t7GYoDkUiEa688kqWL19OUVERe/bs4cwzz2TLli1kZWVRWFjI2rVrOf/88/nHP/5BVlYWmzZtIjMzk7y8POrVq0fnzp1JS0vj8ccfp2/fvgwZMoT27duzbt06Lr/8ct5///3KfkxJkqTTQhAEeWEYZhxaXrUyGiOdjMIw5Je//CWzZ8+mSpUqbNiwgU8++QSAxo0bk5WVBcB7771HdnY2DRo0AKB3796sXr0agJkzZ7Jy5cpYnf/617/YuXMndevWreCnkSRJ0gGGHqnUxIkT2bx5M3l5eVSrVo3ExET27t0LQO3atcucGwTBEesoKSlh3rx51KxZ84S3V5IkScfGOT06/UyaCM0TIaEKdS/rwM6PPwZgx44dfO9736NatWq8/fbbrF279oiXZ2Zm8s4777B161b279/Pyy+/HPusa9euPP7447Hj/Pz8E/ookiRJ+nr29Oj0Mmki3DYYBu6BC+C7BR/R7uEEks47j4svvZRVq1aRkZFBamoqzZs3P2IVDRs25N5776VNmzY0bNiQ9PR0iouLARg7diw33ngjKSkpFBUV0bFjR5566qmKfEJJkiQdwoUMdHppngg91kKLg8pWANMaw6pI5bRJkiRJ5eJoCxk4vE2nl8J1cMEhZReUlkuSJCkuGXp0emnaCAoOKSsoLZckSVJcMvTo9DJyNIyrFR3SVkT0dVytaLkkSZLikgsZ6PTSNyf6OmpEdEhb00YwZvSX5ZIkSYo7hh6dfvrmGHIkSZJOIw5vkyRJkhTXDD2SJEmS4pqhR5IknXTq1KkDwMaNG+nZs+cxny9JR2LokSRJJ61zzjmHqVOnVnYzJJ3iDD2SJOmkFYlESEpKAmD8+PFcc801dOvWjaZNm3LHHXeUOXfEiBG0bNmSrKwsPvnkEwDWrl1Lly5dSElJoUuXLqxb52bU0unI0CNJkk4Z+fn5TJkyhWXLljFlyhTWr18PwO7du8nKymLJkiV07NiRZ555BoChQ4fSr18/li5dSk5ODjfffHNlNl9SJTH0SJKkU0aXLl2oV68eZ5xxBhdddBFr164FoHr16lx55ZUAtGrVikgkAsC8efPo27cvAD/96U+ZM2dOpbRbUuUy9EiSpJPDpInQPBESqsCePdHjQ9SoUSP2PiEhgaKiIgCqVatGEASHlR/qwDmSTi+GHkmSVPkmTYTbBkOPtTAuhKph9Hj6q8dVbdu2bXnxxRcBmDhxIu3bty+P1ko6xVSt7AZIkiQxagQM3AMtSo+rED0eOwZqnvWtqx07diw/+9nP+O1vf0uDBg0YN25cuTRX0qklCMOwstvwtTIyMsLc3NzKboYkSTpREqqU9vAcVFYEDAyguKSyWiXpFBMEQV4YhhmHlju8TZIkVb6mjaDgkLKC0nJJOk6GHkmSVPlGjoZxtWAF0R6eFUSPR46u5IZJigfO6ZEkSZWvb070ddQIKFwX7eEZM/rLckk6DoYeSZJ0cuibY8iRdEI4vE2SJElSXDP0SJIkSYprhh5JkiRJcc3QI0mSJCmuGXokSZIkxTVDjyRJkqS4ZuiRJEmSFNcMPZIkSZLimqFHkiRJUlwz9EiSJEmKa4YeSZIkSXHN0CNJkiQprhl6JEmSJMU1Q48kSZKkuGbokSRJkhTXDD2SJEmS4pqhR5IkSVJcM/RIkiRJimuGHkmSJElxzdAjSZIkKa4ZeiRJkiTFNUOPJEmSpLhm6JEkSZIU1ww9kiRJkuKaoUeSJElSXDP0SJIkSYprhh5JkiRJcc3QI0mSJCmuGXokSZIkxTVDjyRJkqS4ZuiRJEmSFNcMPZIk6ZSQnZ1Nbm7uN77utdde49e//vUJaJGkU8Vxh54gCM4LguDtIAjeD4JgRRAEw0rLvxMEwRtBEBSWvtYvLQ+CIBgbBMGaIAiWBkGQfrxtkCRJ8SEMQ0pKSsq1zh/96Efceeed5VqnpFNLefT0FAH/FYbhhUAWcGMQBBcBdwJvhmHYFHiz9BjgCqBp6T+DgSfLoQ2SJOkUFYlEuPDCCxkyZAjp6ek8//zztGnThvT0dHr16sWuXbsOu6ZOnTqx91OnTmXAgAEApKamxv6pWbMms2bNYvz48QwdOhSA119/nczMTNLS0rj00kv55JNPKuQZJVWu4w49YRhuCsNwUen7ncD7wLnAVcCE0tMmAFeXvr8K+GMYNR84KwiChsfbDkmSdOoqKCigX79+vPHGGzz77LPMnDmTRYsWkZGRwcMPP3zM9eTn55Ofn8/9999PRkYGbdu2LfN5+/btmT9/PosXL6ZPnz489NBD5f0okk5CVcuzsiAIEoE04D3g+2EYboJoMAqC4Hulp50LrD/oso9KyzaVZ1skSdKpo3HjxmRlZfHnP/+ZlStX0q5dOwD27dtHmzZtvlFdhYWF3H777bz11ltUq1atzGcfffQRvXv3ZtOmTezbt48mTZqU2zNIOnmV20IGQRDUAaYBvwjD8F9fdeoRysIj1Dc4CILcIAhyN2/eXF7NlCRJlW3SRGieCAlVoq/TX6V27dpAdE7PZZddFuuxWblyJc8+++xhVQTBl/87sXfv3tj73bt38+Mf/5hnnnmGc84557DrbrrpJoYOHcqyZct4+umny1wrKX6VS+gJgqAa0cAzMQzDP5UWf3Jg2Frp66el5R8B5x10+Q+AjYfWGYbhH8IwzAjDMKNBgwbl0UxJklTZJk2E2wZDj7UwLoy+jr4LduwAICsri7lz57JmzRoA9uzZw+rVqw+r5vvf/z7vv/8+JSUlvPLKK7HygQMHMnDgQDp06HDE2+/YsYNzzz0XgAkTJhzxHEnxpzxWbwuAZ4H3wzA8eNDta0D/0vf9gekHlfcrXcUtC9hxYBicJEmKc6NGwMA90ILoIPsWQK+98Gl0QYEGDRowfvx4fvKTn5CSkkJWVharVq06rJpf//rXXHnllVxyySU0bBidGrx27VqmTp3Kc889F1vM4NAlru+991569epFhw4dOPvss0/ww0o6WQRheNjIsm9WQRC0B94FlgEH1pj8JdF5PS8BjYB1QK8wDD8rDUmPA92APcDAMAy/ctH9jIyM8Nusyy9Jkk4yCVWiPTwHzyouAgYGUFy+S1VLOv0EQZAXhmHGoeXHvZBBGIZzOPI8HYAuRzg/BG483vtKkqRTUNNGULA22sNzQEFpuSSdIOW2kIEkSdLXGjkaxtWCFUR7eFYQPR45upIbppPRwXssScejXJesliRJ+kp9c6Kvo0ZA4bpoD8+Y0V+W67QVhiFhGFKlin+TV/kz9EiSpIrVN8eQE8eGDx9O48aNGTJkCBBdPKJu3bqUlJTw0ksv8cUXX9C9e3fuu+8+IpEIV1xxBZ07d2bevHm8+uqrvPXWWzz44IM0bNiQZs2aUaNGDQA2b97MDTfcwLp16wB49NFHY/s5SV/HKC1JkqRy06dPH6ZMmRI7fumll2jQoAGFhYUsWLCA/Px88vLymD17NgAFBQX069ePxYsXU716de655x7mzp3LG2+8wcqVK2P1DBs2jFtuuYWFCxcybdo0Bg0aVOHPplOXPT2SJEkqN2lpaXz66ads3LiRzZs3U79+fZYuXcqMGTNIS0sDYNeuXRQWFtKoUSMaN25MVlYWAO+99x7Z2dkc2KOxd+/esX2aZs6cWSYE/etf/2Lnzp3UrVu3gp9QpyJDjyRJko7fpImxuVo965/J1Dvv5OMf/IA+ffoQiUS46667uP7668tcEolEqF27dpmy6O4mhyspKWHevHnUrFnzhD2C4pfD2yRJknR8Jk2E2wZDj7UwLqTPNTt48cWJTB03jp49e3L55Zfz3HPPsWvXLgA2bNjAp59+elg1mZmZvPPOO2zdupX9+/fz8ssvxz7r2rUrjz/+eOw4Pz//xD+X4oY9PZIkSTo+o0bAwD2x/ZdaZMPOV0s4d8c2GjZsSMOGDXn//fdp06YNAHXq1OGFF14gISGhTDUNGzbk3nvvpU2bNjRs2JD09HSKi4sBGDt2LDfeeCMpKSkUFRXRsWNHnnrqqYp8Sp3CguheoSe3jIyMMDc3t7KbIUmSpCNJqALjwrJ/Ti8CBgZQXFJZrdJpKAiCvDAMMw4td3ibJEmSjk/TRlBwSFlBabl0EjD0SJLiwqBBg8qs7HSo8ePHs3Hjxq+tJzs7G0cXSN/QyNEwrhasINrDs4Lo8cjRldwwKco5PZKkuPC///u/X/n5+PHjSUpK4pxzzqmgFkmnkQObzZau3kbTRjBmtJvQ6qRhT48k6ZQSiURo3rw5/fv3JyUlhZ49e7Jnz55YD01xcTEDBgwgKSmJ5ORkHnnkEaZOnUpubi45OTmkpqby+eef8+abb5KWlkZycjI/+9nP+OKLLw67189//nMyMjJo0aIF99xzTyU8rXQK6ZsDqyLROTyrIgYenVQMPZKkU05BQQGDBw9m6dKlnHnmmfz+97+PfZafn8+GDRtYvnw5y5YtY+DAgfTs2ZOMjAwmTpxIfn4+QRAwYMAApkyZwrJlyygqKuLJJ5887D6jR48mNzeXpUuXMmvWLJYuXVqRjylJKieGHknSKee8886jXbt2AFx77bXMmTMn9tn555/Phx9+yE033cTf/vY3zjzzzMOuLygooEmTJjRr1gyA/v37M3v27MPOe+mll0hPTyctLY0VK1Z85ZwhSdLJy9AjSTr5TZoIzROjy+J2aU+wZ0+Zj7MC5EoAACAASURBVA/ewb1+/fosWbKE7OxsnnjiCQYNGnRYdceyXcM///lPxowZw5tvvsnSpUv5j//4D/bu3XvcjyJJqniGHknSye2Qnd7ptoF1W7cy7757AZg8eTLt27ePnb5lyxZKSkro0aMH999/P4sWLQKgbt267Ny5E4DmzZsTiURYs2YNAM8//zydOnUqc9t//etf1K5dm3r16vHJJ5/w17/+tQIeVirrqaee4o9//GOF3OtYVziUTkWu3iZJOrkdstM7zeDC78KER/6H66f9iaZNm/Lzn/+c119/HYANGzYwcOBASkqiGyI++OCDAAwYMIAbbriBmjVrMm/ePMaNG0evXr0oKiri4osv5oYbbihz25YtW5KWlkaLFi04//zzY8PppIp06O/yRHKFQ8Wz4Fi6+CtbRkZG6J4JknSaOmSn98hmuPK3sHyTO70r/vzxj39kzJgxBEFASkoK//7v/06dOnW47bbbyM7OJjMzk7fffpvt27fz7LPP0qFDBwYNGhTbW2rDhg0MHTqUe+65h9/+9re89NJLfPHFF3Tv3p377ruPSCTCFVdcQfv27fnHP/7Bueeey/Tp0/m///s/BgwYwLnnnhv7w8A//vEPbrvtttgfBp588klq1KhRyd+Q9NWCIMgLwzDj0HKHt0mSTm5H2un9C9zpXXFnxYoVjB49mrfeeoslS5bw2GOPHXZOUVERCxYs4NFHH+W+++4DontU5efnM336dL773e8yYMAAZsyYQWFhIQsWLCA/P5+8vLzYYh2FhYXceOONrFixgrPOOotp06Z96xUOpVOFoUeSdHI7ZKf3xE9heTV3elf8eeutt+jZsydnn302AN/5zncOO+eaa64BoFWrVkQikVj53r176dWrF48//jiNGzdmxowZzJgxg7S0NNLT01m1ahWFhYUANGnShNTU1CPWc8CxrnAonSqc0yNJOrm507vi2aSJsd92ePZZBG06fuXpB4aXJSQkUFRUFCu/4YYbuOaaa7j00kuB6AqFd911F9dff32Z6yORSJkhagkJCXz++eeH3edUmP4gfRP29EiSTn7u9B53wjCMLTZx2jpkZcIuV23jpT+/xtannwLgs88+O6ZqnnjiCXbu3Mmdd94ZK7v88st57rnn2LVrFxCd6/Ppp59+ZT3fdIVD6VRi6JEkSRUiEolw4YUXMmTIENLT03n++edJTk4mKSmJ4cOHA7B27VqaNm0aW3q8Q4cOzJgxA4CHH36YpKQkkpKSePTRRyvzUcrHwSsTVoUW2TDiipBON99My5YtufXWW4+pmjFjxrBs2TJSU1NJTU3lqaeeomvXrvTt25c2bdqQnJxMz549Y4HmaA6scJiamkoYhrEVDpOTk6lSpUqFriQnlTdXb5MkSRUiEolw/vnn849//INGjRqRlZVFXl4e9evXp2vXrtx8881cffXV/O///i9/+9vfyMzMZM2aNTz99NPk5eUxYMAA5s+fTxiGZGZm8sILL5CWllbZj/XtHbIyIQBFwEBXJpS+LVdvkyQpzuXn5/OXv/ylspvxlRo3bkxWVhYLFy4kOzubBg0aULVqVXJycmIT5QcNGsTOnTt56qmnGDNmDABz5syhe/fu1K5dmzp16nDNNdfw7rvvVuajHL8jrUxYgCsTSieAoUeSpDhxUoaeSROheWK0V6NLe2qXTr7/qpEme/bs4aOPPgKIzUk5FUamfGOHrEzICqLHrkwolTtDjyRJlezAXJfrrruOFi1a0LVrVz7//HPy8/PJysoiJSWF7t27s23bNgCys7Njm1Fu2bKFxMRE9u3bx8iRI5kyZQqpqalMmTKF3bt387Of/YyLL76YtLQ0pk+fXrEPdshEfbptgI83wKSJZGZmMmvWLLZs2UJxcTGTJ0+OTZQfPnw4OTk5jBo1iuuuuw6Ajh078uqrr7Jnzx52797NK6+8QocOHSr2ecpb3xwY8weY1jg6pG1a4+ixC3VI5c7QI0nSSeBIG0b269eP3/zmNyxdupTk5OTYZpRHUr16dUaNGkXv3r3Jz8+nd+/ejB49mksuuYSFCxfy9ttvc/vtt7N79+6Ke6hDJurTDDgrhFEjaNiwIQ8++CCdO3emZcuWpKenc9VVVzFr1iwWLlwYCz7Vq1dn3LhxpKenM2DAAFq3bk1mZiaDBg06tefzHODKhFKFcJ8eSZJOAoduGPnBBx+wffv2WO9H//796dWr1zeqc8aMGbz22muxeTF79+5l3bp1XHjhheXb+KMpXAcXfHmY2ACW/w8wcB0Affv2pW/fvmUu6dSpE/Pnz48d/+lPf4q9v/XWW495RTNJOpihR5KkinbQhpQ0bQQ//8VhG0Zu3779qJdXrVo1tsfN3r17j3peGIZMmzaNCy644KjnnFBNG0HB2mhPzwFO1JdUCRzeJklSRTp0nkuPtTD6Ltixo8xp9erVo379+rEVyg7eHDIxMZG8vDwApk6dGrvm4M0lIbpB5e9+97vYIgCLFy8+oY92GCfqSzpJGHokSapIh85zaQH02guffnLYqRMmTOD2228nJSWF/Px8Ro4cCcBtt93Gk08+Sdu2bdmyZUvs/M6dO7Ny5crYQgZ33303+/fvJyUlhaSkJO6+++6KecYDnKgv6STh5qSSJFUkN6SUpBPGzUklSToZuCGlJFU4Q48kSRXJeS6SVOFcvU2SpIp0YD7Lwau3jRntPBdJOoEMPZIkVbS+OYYcSapADm+TJEmSFNcMPZIkSZLimqFHkiRJUlwz9EiSJEmKa4YeSZIkSXHN0CNJkiQprhl6JEmSJMU1Q48kSZKkuGbokSRJkhTXDD2SJEmS4pqhR5IkSVJcM/RIkiRJimuGHkmSJElxzdAjSZIkKa4ZeiRJkiTFNUOPJEmSpLhm6JEkSZIU1ww9kiRJkuKaoUeSJElSXDP0SJIkSYprhh5JkiRJcc3QI0mSJCmuGXokSZIkxTVDjyRJkqS4ZuiRJEmSFNcMPZIkSZLimqFHkiRJUlwz9EiSJEmKa4YeSZIkSXHN0CNJkiQprhl6JEmSJMU1Q48kSZKkuGbokSRJkhTXDD2SJEmS4pqhR5IkSVJcM/RIkiRJimuGHkmSJElxzdAjSZIkKa4ZeiRJkiTFNUOPJEmSpLhm6JEkSZIU1yot9ARB0C0IgoIgCNYEQXBnZbVDkiRJUnyrlNATBEEC8ARwBXAR8JMgCC6qjLZIkiRJim+V1dPTGlgThuGHYRjuA14ErqqktkiSJEmKY5UVes4F1h90/FFpWUwQBIODIMgNgiB38+bNFdo4SZIkSfGjskJPcISysMxBGP4hDMOMMAwzGjRoUEHNkiRJkhRvKiv0fAScd9DxD4CNldQWSZIkSXGsskLPQqBpEARNgiCoDvQBXquktkiSJEmKY1Ur46ZhGBYFQTAU+DuQADwXhuGKymiLJEmSpPhWKaEHIAzDvwB/qaz7S5IkSTo9VNrmpJIkSZJUEQw9kiRJkuKaoUeSJElSXDP0SJIkSYprhh5JkiRJcc3QI0mSJCmuGXokSZIkxTVDjyRJkqS4ZuiRJEmSFNcMPZIkSZLimqFHkiRJUlwz9EiSJEmKa4YeSZIkSXHN0CNJkiQprhl6JEmSJMU1Q48kSZKkuGbokSRJkhTXDD2SJEmS4pqhR5IkSVJcM/RIkiRJimuGHkmSJElxzdAjSZIkKa4ZeiRJkiTFNUOPJEmSpLhm6JEkSZIU1ww9kiRJkuKaoUeSJElSXDP0SJIkSYprhh5JkiRJcc3QI0mSJCmuGXokSZIkxTVDjyRJkqS4ZuiRJEmSVC7+/ve/k5+fX9nNOIyhR5IkSdJxe+utt/j73/9Oy5Ytv/G1kUiEpKSkE9CqqKonrGZJkiRJp41LLrmESy65pLKbcUT29EiSJEk6Lg8//DBJSUkkJSXx6KOPAvDHP/6RlJQUWrZsyU9/+lMABgwYwNSpU2PX1alT57C69u7dy8CBA0lOTiYtLY23334bgBUrVtC6dWtSU1NJSUmhsLDwmNtnT48kSZKkby0vL49x48bx3nvvEYYhmZmZXHzxxYwePZq5c+dy9tln89lnnx1zfU888QQAy5YtY9WqVXTt2pXVq1fz1FNPMWzYMHJycti3bx/FxcXHXKehR5IkSdK3NmfOHLp3707t2rUBuOaaa8jNzaVnz56cffbZAHznO9/5RvXddNNNADRv3pzGjRuzevVq2rRpw+jRo/noo4+45ppraNq06THX6fA2SZIkSd/MpInQPBESqhCOvg+WLyvzcRAEBEFw2GVVq1alpKQEgDAM2bdv32HnhGF4xFv27duX1157jZo1a3L55Zfz1ltvHXNzDT2SJJ0ETvTKRZJUbiZNhNsGQ4+1MC6k4//bxquvT2fPuOfYvXs3r7zyCq1ateKll15i69atALHhbYmJieTl5QEwffp09u/ff1j1HTt2ZOLEiQCsXr2adevWccEFF/Dhhx9y/vnnc/PNN/OjH/2IpUuXHnOTDT2SJEmSjt2oETBwD7QAqkJ6FxjQKaT1DTeQmZnJoEGDaNeuHSNGjKBTp060bNmSW2+9FYDrrruOWbNm0bp1a957773YkLiDDRkyhOLiYpKTk+nduzfjx4+nRo0aTJkyhaSkJFJTU1m1ahX9+vU75iYHR+s+OplkZGSEubm5ld0MSdJp7uqrr2b9+vXs3buXYcOGMXjwYOrUqcOwYcP485//TM2aNZk+fTrf//73+eCDD8jJyaG4uJgrrriChx9+mF27dhGGIXfccQd//etfCYKAX/3qV/Tu3ZtIJMKVV17J8uXLiUQi/PSnP2X37t0APP7447Rt27aSn16SSiVUgXFh2dUBioCBARSXVFarAAiCIC8Mw4xDy+3pkSTpGD333HPk5eWRm5vL2LFj2bp1K7t37yYrK4slS5bQsWNHnnnmGQCGDRvGsGHDWLhwIeecc06sjj/96U/k5+ezZMkSZs6cye23386mTZvK3Od73/seb7zxBosWLWLKlCncfPPNFfqckvSVmjaCgkPKCkrLT1KGHkmSjtHYsWNp2bIlWVlZrF+/nsLCQqpXr86VV14JQKtWrYhEIgDMmzePXr16AdHJtwfMmTOHn/zkJyQkJPD973+fTp06sXDhwjL32b9/P9dddx3Jycn06tWLlStXVswDStKxGDkaxtWCFUR7eFYQPR45upIbdnQuWS1J0tFMmhgdu164jnfO/R4za9Zj3uLF1KpVi+zsbPbu3Uu1atViKxQlJCRQVFT0lVUey7DyRx55hO9///ssWbKEkpISzjjjjHJ5HEkqF31zoq+l/32kaSMYM/rL8pOQPT2SJB3JIasT7cj8hPpr11Dr1VdYtWoV8+fP/8rLs7KymDZtGgAvvvhirLxjx45MmTKF4uJiNm/ezOzZs2ndunWZa3fs2EHDhg2pUqUKzz///DfagE+SKkTfHFgVic7hWRU5qQMPGHokSTqyQ1Yn6nYlFJ1bQsrAgdx9991kZWV95eWPPvooDz/8MK1bt2bTpk3Uq1cPgO7du5OSkkLLli255JJLeOihh/i3f/u3MtcOGTKECRMmkJWVxerVq4+4upEk6di5epskSUdynKsT7dmzh5o1axIEAS+++CKTJ09m+vTpJ6y5kqSjr97mnB5Jko6kaSMoWBvt6TngG6xOlJeXx9ChQwnDkLPOOovnnnvuhDRTkvT1DD2SJB3JyNHROT0D98AFRAPPuFrRybrHoEOHDixZsuSENlFSxdmwYQNvv/021157bWU3Rd+Cc3okSTqSvjkw5g8wrXF0SNu0xtHjk3yyrqRvZvz48WzcuPFrz7v11ltJSUmJHQ8YMICpU6cedl5ubq57a52E7OmRJOlo+uYYcqQ4N378eJKSkspsInxAcXExCQkJbNq0if/8z/8sE3qOJiMjg4yMw6aUqJLZ0yNJkqS488ILL9C6dWtSU1O5/vrrKS4uZsCAASQlJZGcnMwjjzzC1KlTyc3NJScnh9TUVD7//HMSExMZNWoU7du35+WXX+aZZ57hRz/6Ebfddhs9evRgz549sXvMnDmTDh060KxZM/785z8D8M4778Q2LN61axcDBw4kOTmZlJSU2DL2kydPJjk5maSkJIYPH17xX85pyJ4eSZIkxZX333+fKVOmMHfuXKpVq8aQIUN44IEH2LBhA8uXLwdg+/btnHXWWTz++OOMGTOmTO/MGWecwZw5cwDYunUr1113HQB33XUXzz77LDfddBMAkUiEWbNm8cEHH9C5c2fWrFlTph33338/9erVY9myZQBs27aNjRs3Mnz4cPLy8qhfvz5du3bl1Vdf5eqrrz7h38vpzJ4eSZIkxZU333yTvLw8Lr74YlJTU3nzzTf57LPP+PDDD7npppv429/+xplnnnnU63v37h17//7779O1a1c6dOjAa6+9xooVK2Kf/fjHP6ZKlSo0bdqU888/n1WrVpWpZ+bMmdx4442x4/r167Nw4UKys7Np0KABVatWJScnh9mzZ5fj0+tIDD2SJEk69U2aCM0TIaEK4f0j6Z+RQX5+Pvn5+RQUFPDYY4+xZMkSsrOzeeKJJxg0aNBRqzp4Q+B+/frx2GOP8e6773Lbbbexd+/e2GdBEJS57tDjMAyPWKaKZ+iRJEnSqW3SxOgS8z3WwriQLldtY+pf/synT/4egM8++4y1a9dSUlJCjx49uP/++1m0aBEAdevWZefOnUeteseOHXz3u99l//79TJw4scxnL7/8MiUlJXzwwQd8+OGHXHDBBWU+79q1K48//njseNu2bWRmZjJr1iy2bNlCcXExkydPplOnTuX1TegonNMjSZKkU9uoEdE9tUo3E74oGx7YHNL1ll9Q8uRTVKtWjYcffpju3btTUlICwIMPPghEl56+4YYbqFmzJvPmzTu86lGjyMzMpHHjxiQnJ5cJSBdccAGdOnXik08+4amnnuKMM84oc+2vfvUrbrzxRpKSkkhISOCee+7hmmuu4cEHH6Rz586EYcgPf/hDrrrqqhPzvSgmOBW62DIyMsLc3NzKboYkSZJORglVYFxY9s/5RUT32CouqaxWqRIEQZAXhuFha4Y7vE2SJEmntqaNoOCQsoLScglDjyRJkk51I0fDuFqwgmgPzwqixyNHV3LDdLJwTo8kSZJObX1zoq+jRkDhumgPz5jRX5brtGfokSRJ0qmvb44hR0fl8DZJkiRJcc3QI0mSJCmuGXokSZIklVFcXFzZTShXhh5JkiTpFPLCCy/QunVrUlNTuf766ykuLmbAgAEkJSWRnJzMI488AsCaNWu49NJLadmyJenp6XzwwQe88847XHnllbG6hg4dyvjx4wFITExk1KhRtG/fnpdffpn8/HyysrJISUmhe/fubNu2DYCxY8dy0UUXkZKSQp8+fSr8+b8NFzKQJEmSThHvv/8+U6ZMYe7cuVSrVo0hQ4bwwAMPsGHDBpYvXw7A9u3bAcjJyeHOO++ke/fu7N27l5KSEtavX/+V9Z9xxhnMmTMHgJSUFH73u9/RqVMnRo4cyX333cejjz7Kr3/9a/75z39So0aN2L1Odvb0SJLK3fjx49m4ceMJqTsxMZEtW7ackLol6WT35ptvkpeXx8UXX0xqaipvvvkmn332GR9++CE33XQTf/vb3zjzzDPZuXMnGzZsoHv37kA0zNSqVetr6+/duzcAO3bsYPv27XTq1AmA/v37M3v2bCAahnJycnjhhReoWvXU6EMx9EiSylVxcfEJDT2SdNqZNBGaJ0JCFcL7R9I/I4P8/Hzy8/MpKCjgscceY8mSJWRnZ/PEE08waNAgwjA8YlVVq1alpKQkdrx3794yn9euXftrm/N///d/3HjjjeTl5dGqVSuKioqO6/EqgqFHknSYSCRC8+bN6d+/PykpKfTs2ZM9e/bw5ptvkpaWRnJyMj/72c/44osvgLLjwCdPnkxubi45OTmkpqby+eefl+mdyc3NJTs7G4DNmzdz2WWXkZ6ezvXXX0/jxo1j51199dW0atWKFi1a8Ic//KFSvgdJqnSTJsJtg6HHWhgX0uWqbUz9y5/59MnfA/DZZ5+xdu1aSkpK6NGjB/fffz+LFi3izDPP5Ac/+AGvvvoqAF988QV79uyhcePGrFy5ki+++IIdO3bw5ptvHvG29erVo379+rz77rsAPP/883Tq1Ck2RK5z58489NBDbN++nV27dlXMd3EcDD2SpCMqKChg8ODBLF26lDPPPJOHH36YAQMGMGXKFJYtW0ZRURFPPvlk7PwD48CvvfZaMjIymDhxIvn5+dSsWfOo97jvvvu45JJLWLRoEd27d2fdunWxz5577jny8vLIzc1l7NixbN269YQ+rySdlEaNgIF7oAVQFS7Khgf+X0jXW35BSkoKl112GZFIhOzsbFJTUxkwYAAPPvggEA0qY8eOJSUlhbZt2/Lxxx9z3nnn8eMf/zg2RC0tLe2ot54wYQK33347KSkp5OfnM3LkSIqLi7n22mtJTk4mLS2NW265hbPOOqtivovjcGoMwpMkVbjzzjuPdu3aAXDttddy//3306RJE5o1awZEx3c/8cQT/OIXvwC+HAf+TcyZM4dXXnkFgG7dulG/fv3YZ2PHjo19tn79egoLC/nud797XM8kSaecwnVwQdmi3t2h92tFsHRprGzRokWHXdq0aVPeeuutw8ofeughHnroocPKI5FImePU1FTmz59/2HkHFjo4ldjTI0k6oiAIvtH5XzUO/OAx5AePHz/amPN33nmHmTNnMm/ePJYsWUJaWtph484l6bTQtBEUHFJWUFquY2bokSRFHTRRli7tWbduHfPmzQNg8uTJXHrppUQiEdasWQN8Ob77SOrWrcvOnTtjx4mJieTl5QEwbdq0WHn79u156aWXAJgxY0ZsD4gdO3ZQv359atWqxapVq474l0ZJOi2MHA3jasEKoIjo67ha0XIdM0OPJOmwibJ028CFVQMm/OpXpKSk8Nlnn3HLLbcwbtw4evXqRXJyMlWqVOGGG244YnUD/v/27j46i+pe9Ph3EwRF7RWa9h4UFVwrNEoSCk1EEQgVFXrq1QpUEUTkHEQoXCyW1iJH0bS4rOVaZdmCUEWxUEEtFvVUgVKLcBEJBXlRqNSG+NYrCkXlTQL7/vFMYsAErBCSPPl+1pqVmd/seWYPe80Tftl79lx3HcOGDauYyGD8+PHceOONdO3alYyMjIpy48ePZ/78+XTs2JE//OEPtGzZkpNPPplevXpRVlZGXl4et956K+edd96x+peQpLql/wCYOBWePBMGh9TPiVNTcX1uobqhBXVJfn5+LC4uru1qSFL6ym6dSnjapTZLtsClP4V1Xz4TNpTU2Gn37NlDRkYGjRs3ZtmyZQwfPpzVq1fX2PkkSekthLAyxph/cNyJDCRJVT4oS9MkXoNKS0u58sor2b9/P02aNGHatGk1ej5JUsN0RElPCOHnwP8CPgH+BgyOMf4z2TcW+E9gHzAqxvh8Eu8F3AdkAL+OMd51JHWQJB0FWWfAxk97elp/BdYNAp6s2Qdls7KyWLVqVY2eQ5KkI32mZwGQE2PMA/4KjAUIIZwD9CP167MX8KsQQkYIIQP4JfAt4Bzg6qSsJKk2+aCsJCmNHVFPT4xxfqXNl4C+yfrlwGMxxj3A30MIm4Bzk32bYoxvAIQQHkvKvnok9ZAkHaHyB2KLxqWGtGWdARMn+KCsJCktHM1nev4DmJ2sn0YqCSr3VhIDePOgeKejWAdJ0hfVf4BJjiQpLR026QkhLAT+rYpd42KMv0/KjCM1IGJm+WFVlI9UPZyuyunjQghDgaEAZ5zhy5ckSZIkfTGHTXpijBcdan8IYRBwKdAjfjr/9VvA6ZWKtQLeSdarix983qnAVEhNWX24ekqSJElSVY5oIoNkJrabgctijDsr7ZoH9AshNA0htAGygJeBFUBWCKFNCKEJqckO5h1JHSRJkiTpUI70mZ77Sb3JYUEIAeClGOOwGOP6EMIcUhMUlAEjYoz7AEIII4HnSU1Z/VCMcf0R1kGSJEmSqhU+HZFWd+Xn58fi4uLaroYkSZKkOiyEsDLGmH9w/Ejf0yNJkiRJdZpJjyRJkqS0ZtIjSZIkKa2Z9EiSJElKayY9kiRJktKaSY8kSZKktGbSI0mSJCmtmfRIkiRJSmsmPZIkSZLSmkmPJEmSpLRm0iNJkiQprZn0SJIkSUprJj2SJEmS0ppJjyRJkuq0GTNmkJeXR/v27Rk4cCBbtmyhT58+FBQUUFBQwNKlSwHYsmULF198MR07duSGG27gzDPP5P333wfgnnvuIScnh5ycHO69997avBzVgsa1XQFJkiSpOuvXr2fChAksXbqUzMxMtm7dysiRIxk9ejRdunShtLSUnj178tprr3HHHXdw4YUXMnbsWJ577jmmTp0KwMqVK5k+fTrLly8nxkinTp0oLCykQ4cOtXx1OlZMeiRJklRnLVq0iL59+5KZmQlAixYtWLhwIa+++mpFmQ8//JCPPvqIJUuWMHfuXAB69epF8+bNAViyZAlXXHEFJ554IgC9e/fmxRdfNOlpQEx6JEmSVLfMmglF4+D1UmLmKYTzux2we//+/SxbtowTTjjhgHiMscqPqy6uhsNneiRJklR3zJoJY4ZCn80wPdLj8m3MeWYeHzwwBYCtW7dyySWXcP/991ccsnr1agC6dOnCnDlzAJg/fz7btm0DoFu3bjz11FPs3LmTHTt2MHfuXLp27XqML0y1KdSHzDc/Pz8WFxfXdjUkSZJU07JbpxKedp+GHvkt/Hz+cWRkn02HDh2YOHEiI0aM4LXXXqOsrIxu3boxZcoU3nvvPa6++mq2bdtGYWEhs2fP5u9//ztNmzblnnvu4aGHHgJgyJAhfP/736+d61ONCiGsjDHmfyZu0iNJkqQ6I6MRTI8HPoRRBgwOsG//IQ/ds2cPGRkZNG7cmGXLljF8+PCKzkfq1QAAFfpJREFUXiA1DNUlPT7TI0mSpLoj6wzYeGBPDxuT+GGUlpZy5ZVXsn//fpo0acK0adNqrJqqX0x6JEmSVHfcNiH1TM/gnfA1UgnP9GYwccJhD83KymLVqlU1XkXVPyY9kiRJqjv6D0j9TGZvI+uMVMJTHpe+AGdvkyRJUt3SfwBsKEk9w7OhxISnDpgxYwZ5eXm0b9+egQMHsmXLFvr06UNBQQEFBQUsXboUgC1btnDxxRfTsWNHbrjhBs4880zef/99AO655x5ycnLIycnh3nvvBWDHjh18+9vfpn379uTk5DB79uwaqb89PZIkSZKqtX79eiZMmMDSpUvJzMxk69atjBw5ktGjR9OlSxdKS0vp2bMnr732GnfccQcXXnghY8eO5bnnnmPq1KkArFy5kunTp7N8+XJijHTq1InCwkLeeOMNTj31VJ599lkAtm/fXiPXYNIjSZIkqVqLFi2ib9++ZGZmAtCiRQsWLlzIq6++WlHmww8/5KOPPmLJkiXMnTsXgF69etG8eXMAlixZwhVXXMGJJ54IQO/evXnxxRfp1asXY8aM4eabb+bSSy+tsfcnObxNkiRJ0qdmzUy9LymjEWS3Jq5YQQjhgCL79+9n2bJlrF69mtWrV/P2229z8sknU93rcKqLt23blpUrV5Kbm8vYsWMpKio62lcDmPRIkiRJKjdrZmr2vD6bU+9L6rOZHn94nDkPPsgHH3wAwNatW7nkkku4//77Kw4rfx9Sly5dmDNnDgDz589n27ZtAHTr1o2nnnqKnTt3smPHDubOnUvXrl155513aNasGddccw1jxozhL3/5S41clsPbJEmSJKUUjUtNF17+nqR20G7obsY9tJfCwkIyMjLo0KEDkyZNYsSIEeTl5VFWVka3bt2YMmUK48eP5+qrr2b27NkUFhbSsmVLTj75ZDp27Mh1113HueeeC8CQIUPo0KEDzz//PD/84Q9p1KgRxx13HJMnT66RywrVdTXVJfn5+bG4uLi2qyFJkiSlt4xGqR6eyl0jZcDgkJpN7zD27NlDRkYGjRs3ZtmyZQwfPryiF+hYCCGsjDHmHxy3p0eSJElSStYZsHHzpz09kHpBbNYZn+vw0tJSrrzySvbv30+TJk2YNm1ajVTzX2XSI0mSJCnltgmpZ3oG74SvkUp4pjdLvSD2c8jKymLVqlU1WsUvwqRHkiRJUkr5i2CLxsHrpakenokT6v0LYk16JEmSJH2q/4B6n+QczCmrJUmSJKU1kx5JkiRJac2kR5KqMWXKFGbMmPG5y5eUlJCTk1ODNZIkSV+Ez/RIUjWGDRtWI5+7b98+MjIyauSzJUnSZ9nTI6nBmTFjBnl5ebRv356BAweyefNmevToQV5eHj169KC0tBSA22+/nYkTJwLQvXt3Ro8eTbdu3Tj77LNZsWIFvXv3Jisri//6r/+q+OyysjIGDRpEXl4effv2ZefOnQC0bt2aoqIiunTpwuOPP860adMoKCigffv29OnTp6Kc0su/2lsoSaoZJj2SGpT169czYcIEFi1axCuvvMJ9993HyJEjufbaa1mzZg0DBgxg1KhRVR7bpEkTFi9ezLBhw7j88sv55S9/ybp163j44Yf54IMPANi4cSNDhw5lzZo1fOlLX+JXv/pVxfHHH388S5YsoV+/fvTu3ZsVK1bwyiuvcPbZZ/Pggw8ek+vXsVNWVsawYcO49tpra7sqktTgmfRIalAWLVpE3759yczMBKBFixYsW7aM/v37AzBw4ECWLFlS5bGXXXYZALm5ubRr146WLVvStGlTzjrrLN58800ATj/9dC644AIArrnmmgM+66qrrqpYX7duHV27diU3N5eZM2eyfv36o3+xOmIlJSVkZ2d/pveuqKiIgoICcnJyGDp0KDFGINUjeMstt1BYWMh99913QG/hpEmTOOecc8jLy6Nfv361eVmS1OCY9EhKf7NmQnZryGhE/Ol4wmESjBBClfGmTZsC0KhRo4r18u2ysrIqj628feKJJ1asX3fdddx///2sXbuW8ePHs3v37n/pknTsVNV7N3LkSFasWMG6devYtWsXzzzzTEX5f/7zn/z5z3/mBz/4wQGfc9ddd7Fq1SrWrFnDlClTjvVlSFKDZtIjKb3NmgljhkKfzTA90uPybcx5Zh4fPJD6T+fWrVvp3Lkzjz32GAAzZ86kS5cuX/h0paWlLFu2DIDf/va31X7WRx99RMuWLdm7dy8zZ878wudTzauq9+5Pf/oTnTp1Ijc3l0WLFh3QU1e5R6+yvLw8BgwYwG9+8xsaN3YeIUk6lkx6JKW3onEweCe0AxpDu+4w7luRwlGjaN++PTfddBOTJk1i+vTp5OXl8eijj3Lfffd94dOdffbZPPLII+Tl5bF161aGDx9eZbmf/OQndOrUiYsvvpjs7OwvfD7VgEo9g/ToQjhokokQAt/73vd44oknWLt2Lddff/0BPXWVe/Qqe/bZZxkxYgQrV67kG9/4RkXvoCSp5oXycch1WX5+fiwuLq7takiqjzIawfR44AT9ZcDgAPv211atjrmTTjqJjz/+mHfeeYdRo0bxxBNPVFmupKSESy+9lHXr1h3jGtYR5T2Dg3fC16BkObT5Ffzf28dz/vjbuf7668nOzubuu++mpKSEffv2cd5559G3b19uv/12unfvzsSJE8nPzwdSMwCedNJJ3HTTTZSWltK6dWv27t1Lq1at2LhxI6ecckotX7AkpZcQwsoYY/7BcfvXJaW3rDNg4+ZUT0+5jUm8ATr11FOrTXjEgT2DAG3h7C/DI7/4P9zw5O/Iyspi+PDhbNu2jdzcXFq3bk1BQcFhP3bfvn1cc801bN++nRgjo0ePNuGRpGPInh5J6e2gv9yzEZjeDCZOhf4Dart2x0x5T0/lnpz169czePBgPvnkE/bv38+TTz7JcccdR69evejUqROrVq2ibdu2zJgxg2bNmvHHP/6RMWPGUFZWRkFBAZMnT6Zp06YUFRXx9NNPs2vXLjp37swDDzxQ7WQQdd5BPYMlW+DSn8O6dxtWz6Ak1VfV9fT4TI+k9NZ/QCrBefLM1JC2J89scAlPdaZMmcKNN97I6tWrKS4uplWrVkDVs5Xt3r2b6667jtmzZ7N27VrKysqYPHkywCFnMqt3ss5IJcaV7aHB9gxKUrow6ZGU/voPgA0lqb/Ubygx4Umcf/753HnnnfzsZz9j8+bNnHDCCUDVs5Vt3LiRNm3a0LZtWwAGDRrE4sWLAQ45k1m9c9uEVE/geqAMWr8H645rlopLkuotkx5JSleVZyHbuTO1XUn//v2ZN28eJ5xwAj179mTRokVA1e8aqm4o9O7duw85k1m9Y8+gJKUlkx5JSkcHvZ+IxjG1/funKoq88cYbnHXWWYwaNYrLLruMNWvWAFW/ayg7O5uSkhI2bdoEwKOPPkphYWFFgpOZmcnHH3+cHpMk2DMoSWnHpEeS0tFB7yeiEantSRMrisyePZucnBy+/vWvs2HDBq699lqg6ncNHX/88UyfPp3vfve75Obm0qhRI4YNG8Ypp5zC9ddfT25uLt/5znc+10xmkiQda87eJknpyPcTSZIaIGdvk6SGpKpZyBrw+4kkSQ2bSY8kpaODZiFjPaltZyGTJDVAjQ9fRJJU75Q/fF80Dl4vTfXwTJzgQ/mSpAbJpEeS0lX/ASY5kiTh8DZJkiRJac6kR5IkSVJaM+mRJEmSlNZMeiRJkiSlNZMeSZIkSWnNpEeSJElSWjPpkSRJkpTWTHokSZIkpTWTHkmSJElpzaRHkiRJUloz6ZEkSZKU1kx6JEmSJKU1kx5JkiRJac2kR5IkSVJaM+mRJEmSlNZMeiRJkiSlNZMeSZIkSWnNpEeSJElSWjPpkSR9Yd27d6e4uLjKfUOGDOHVV1/9TPzhhx9m5MiRNV01SZIqNK7tCkiS0s++ffv49a9/XdvVkCQJsKdHkhqckpISsrOzGTJkCDk5OQwYMICFCxdywQUXkJWVxcsvv8zLL79M586d6dChA507d2bjxo0A7Nq1i379+pGXl8dVV13Frl27Kj73pJNO4rbbbqNTp04sW7bsgF6g6dOn07ZtWwoLC1m6dGmtXLckqeE6KklPCGFMCCGGEDKT7RBCmBRC2BRCWBNC6Fip7KAQwuvJMuhonF+S9K/ZtGkTN954I2vWrGHDhg3MmjWLJUuWMHHiRO68806ys7NZvHgxq1atoqioiFtuuQWAyZMn06xZM9asWcO4ceNYuXJlxWfu2LGDnJwcli9fTpcuXSri7777LuPHj2fp0qUsWLCgyiFvkiTVpCMe3hZCOB24GCitFP4WkJUsnYDJQKcQQgtgPJAPRGBlCGFejHHbkdZDkvT5tWnThtzcXADatWtHjx49CCGQm5tLSUkJ27dvZ9CgQbz++uuEENi7dy8AixcvZtSoUQDk5eWRl5dX8ZkZGRn06dPnM+davnw53bt35ytf+QoAV111FX/9619r+hIlSapwNHp6fgH8iFQSU+5yYEZMeQk4JYTQEugJLIgxbk0SnQVAr6NQB0nSocyaCdmtIaMR9OhC0927K3Y1atSIpk2bVqyXlZVx66238s1vfpN169bx9NNPs7tS+RBClac4/vjjycjIqHJfdcdIknQsHFHSE0K4DHg7xvjKQbtOA96stP1WEqsuLkmqKbNmwpih0GczTI/Q6234x9upeDW2b9/Oaaelvp4ffvjhini3bt2YOTN13Lp161izZs1hT9+pUydeeOEFPvjgA/bu3cvjjz9+ZNcjSdK/6LBJTwhhYQhhXRXL5cA44LaqDqsiFg8Rr+q8Q0MIxSGE4i1bthyumpKk6hSNg8E7oR2pQc1tgVNiKl6NH/3oR4wdO5YLLriAffv2VcSHDx/Oxx9/TF5eHnfffTfnnnvuYU/fsmVLbr/9ds4//3wuuugiOnbseNhjJEk6mkKMVeYchz8whFzgj8DOJNQKeAc4F7gDeCHG+Nuk7Eage/kSY7whiT9QuVx18vPzY3XvgZAkHUZGo1QPT+WnOMuAwQH27a+tWkmSdNSFEFbGGPMPjn/h4W0xxrUxxq/GGFvHGFuTGqrWMcb4D2AecG0yi9t5wPYY47vA88AlIYTmIYTmwCVJTJJUU7LOgI0HxTYmcUmSGoCaek/PfwNvAJuAacD3AGKMW4GfACuSpSiJSZJqym0TYHozWE+qh2c9qe3bJtRyxSRJOjaOeMrqcklvT/l6BEZUU+4h4KGjdV5J0mH0H5D6WTQOXi9N9fBMnPBpXJKkNHfUkh5JUh3Wf4BJjiSpwaqp4W2SJEmSVCeY9EiSJElKayY9kiRJktKaSY8kSZKktGbSI0mSJCmtmfRIkiRJSmsmPZIkSZLSmkmPJEmSpLRm0iNJkiQprZn0SJIkSUprJj2SJEmS0ppJjyRJkqS0ZtIjSZIkKa2Z9EiSJElKayY9kiRJktJaiDHWdh0OK4SwBdhc2/VooDKB92u7Ejos26l+sJ3qB9upfrCd6gfbqX5Ip3Y6M8b4lYOD9SLpUe0JIRTHGPNrux46NNupfrCd6gfbqX6wneoH26l+aAjt5PA2SZIkSWnNpEeSJElSWjPp0eFMre0K6HOxneoH26l+sJ3qB9upfrCd6oe0byef6ZEkSZKU1uzpkSRJkpTWTHpUIYTw9RDCSyGE1SGE4hDCuUk8hBAmhRA2hRDWhBA6VjpmUAjh9WQZVHu1bzhCCP87hLAxhLA+hHB3pfjYpI02hhB6Vor3SmKbQgg/rp1aN1whhDEhhBhCyEy2vZ/qiBDCz0MIG5J2mBtCOKXSPu+nOso2qDtCCKeHEP4UQngt+Z10YxJvEUJYkHyXLQghNE/i1X7/qeaFEDJCCKtCCM8k221CCMuTdpodQmiSxJsm25uS/a1rs95HTYzRxYUYI8B84FvJ+r8DL1Ra/wMQgPOA5Um8BfBG8rN5st68tq8jnRfgm8BCoGmy/dXk5znAK0BToA3wNyAjWf4GnAU0ScqcU9vX0VAW4HTgeVLvGctMYt5PdWQBLgEaJ+s/A36WrHs/1dHFNqhbC9AS6Jisnwz8Nbl/7gZ+nMR/XOneqvL7z+WYtddNwCzgmWR7DtAvWZ8CDE/WvwdMSdb7AbNru+5HY7GnR5VF4EvJ+v8A3knWLwdmxJSXgFNCCC2BnsCCGOPWGOM2YAHQ61hXuoEZDtwVY9wDEGN8L4lfDjwWY9wTY/w7sAk4N1k2xRjfiDF+AjyWlNWx8QvgR6TurXLeT3VEjHF+jLEs2XwJaJWsez/VXbZBHRJjfDfG+Jdk/SPgNeA0Um3ySFLsEeA7yXp133+qYSGEVsC3gV8n2wG4EHgiKXJwO5W33xNAj6R8vWbSo8q+D/w8hPAmMBEYm8RPA96sVO6tJFZdXDWnLdA16W7+cwihIInbRnVMCOEy4O0Y4ysH7bKt6qb/IPUXaLCN6jLboI5KhkB1AJYD/zPG+C6kEiPgq0kx26/23Evqj3D7k+0vA/+s9Iefym1R0U7J/u1J+XqtcW1XQMdWCGEh8G9V7BoH9ABGxxifDCFcCTwIXESqG/pg8RBxHYHDtFFjUkOfzgMKgDkhhLOovi2q+sOGbXSUHKatbiE1fOozh1UR836qIYdqoxjj75My44AyYGb5YVWU936qG7xP6qAQwknAk8D3Y4wfHqJTwParBSGES4H3YowrQwjdy8NVFI2fY1+9ZdLTwMQYL6puXwhhBnBjsvk4SRcoqez/9EpFW5Ea+vYW0P2g+AtHqaoN1mHaaDjwu5gaaPtyCGE/kEn1bcQh4jpC1bVVCCGX1LMgryS//FsBf0kmB/F+OoYOdT9BavII4FKgR3JfgfdTXXaotlEtCCEcRyrhmRlj/F0S/n8hhJYxxneT4WvlQ7Ftv9pxAXBZCOHfgeNJPcpwL6nhhY2T3pzKbVHeTm+FEBqTeuRh67Gv9tHl8DZV9g5QmKxfCLyerM8Drk1mXTkP2J50Vz8PXBJCaJ7MzHJJElPNeYpU2xBCaEvqQd73SbVRv2TGlTZAFvAysALISmZoaULqgcR5tVLzBiTGuDbG+NUYY+sYY2tSv0A6xhj/gfdTnRFC6AXcDFwWY9xZaZf3U91lG9QhyXMeDwKvxRjvqbRrHlA+A+Ug4PeV4lV9/6kGxRjHxhhbJb+P+gGLYowDgD8BfZNiB7dTefv1Tcrb06O0cj1wX5LV7waGJvH/JjXjyiZgJzAYIMa4NYTwE1K/hACKYoz1/i8BddxDwEMhhHXAJ8Cg5ItofQhhDvAqqWE6I2KM+wBCCCNJ/ec5A3goxri+dqquhPdT3XE/qRnaFiQ9ci/FGIfFGL2f6qgYY5ltUKdcAAwE1oYQViexW4C7SA2//k+gFPhusq/K7z/VmpuBx0IIPwVWkUpgSX4+GkLYRKqHp18t1e+oCmmQuEmSJElStRzeJkmSJCmtmfRIkiRJSmsmPZIkSZLSmkmPJEmSpLRm0iNJkiQprZn0SJIkSUprJj2SJEmS0ppJjyRJkqS09v8B1ksPWCzqXboAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 1008x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "words = sum([[k] + v for k, v in similar_words.items()], [])\n",
    "wvs = w2v_model.wv[words]\n",
    "tsne = TSNE(n_components=2, random_state=0, n_iter=10000, perplexity=2)\n",
    "np.set_printoptions(suppress=True)\n",
    "T = tsne.fit_transform(wvs)\n",
    "labels = words\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.scatter(T[:, 0], T[:, 1], c='orange', edgecolors='r')\n",
    "for label, x, y in zip(labels, T[:, 0], T[:, 1]):\n",
    "    plt.annotate(label, xy=(x+1, y+1), xytext=(0, 0), textcoords='offset points')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(np.array(df_train_balanced.loc[:,'simple_clean']),\n",
    "                                                    np.array(df_train_balanced.loc[:,'author_name']),\n",
    "                                                    test_size=0.20,\n",
    "                                                    random_state=42,\n",
    "                                                    stratify=df_train_balanced.loc[:,'author_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train = [nltk.WordPunctTokenizer().tokenize(document) for document in X_train]\n",
    "tokenized_test = [nltk.WordPunctTokenizer().tokenize(document) for document in X_test]\n",
    "tokenized_final_test = [nltk.WordPunctTokenizer().tokenize(document) for document in df_test.loc[:,'simple_clean']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_vectorizer(corpus, model, num_features):\n",
    "    vocabulary = set(model.wv.index2word)\n",
    "    \n",
    "    def average_word_vectors(words, model, vocabulary, num_features):\n",
    "        feature_vector = np.zeros((num_features,), dtype=\"float64\")\n",
    "        nwords = 0.\n",
    "        for word in words:\n",
    "            if word in vocabulary:\n",
    "                nwords = nwords + 1.\n",
    "                feature_vector = np.add(feature_vector, model.wv[word])\n",
    "        if nwords:\n",
    "            feature_vector = np.divide(feature_vector, nwords)\n",
    "        return feature_vector\n",
    "    \n",
    "    features = [average_word_vectors(tokenized_sentence, model, vocabulary, num_features) \n",
    "                for tokenized_sentence in corpus]\n",
    "    return np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow_train = document_vectorizer(corpus=tokenized_train, model=w2v_model, num_features=100)\n",
    "cbow_test = document_vectorizer(corpus=tokenized_test, model=w2v_model, num_features=100)\n",
    "cbow_final_test = document_vectorizer(corpus=tokenized_final_test, model=w2v_model, num_features=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([('classifier', MultinomialNB())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = [{'classifier': [MultinomialNB()],\n",
    "                 'classifier__alpha': [1e-5, 1e-4, 1e-2, 1e-1, 1]},\n",
    "                {'classifier': [LogisticRegression(penalty='l2', max_iter=100, random_state=42)],\n",
    "                 'classifier__C': [1, 5, 10]},\n",
    "                {'classifier': [LinearSVC(penalty='l2', random_state=42)],\n",
    "                 'classifier__C': [0.01, 0.1, 1, 5]},\n",
    "                {'classifier': [SGDClassifier(loss='hinge', penalty='l2', max_iter=5, random_state=42)]},\n",
    "                {'classifier': [RandomForestClassifier(n_estimators=10, random_state=42)]},\n",
    "                {'classifier': [GradientBoostingClassifier(n_estimators=10, random_state=42)]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GridSearchCV(pipe, search_space, cv=5, verbose=2, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 15 candidates, totalling 75 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=-1)]: Done  75 out of  75 | elapsed:    3.8s finished\n"
     ]
    }
   ],
   "source": [
    "clf = clf.fit(cbow_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_estimator_.get_params()[\"classifier\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = clf.predict(cbow_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AlmadaNegreiros</th>\n",
       "      <th>CamiloCasteloBranco</th>\n",
       "      <th>EcaDeQueiros</th>\n",
       "      <th>JoseRodriguesSantos</th>\n",
       "      <th>JoseSaramago</th>\n",
       "      <th>LuisaMarquesSilva</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>AlmadaNegreiros</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>CamiloCasteloBranco</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>EcaDeQueiros</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>JoseRodriguesSantos</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>JoseSaramago</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>LuisaMarquesSilva</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     AlmadaNegreiros  CamiloCasteloBranco  EcaDeQueiros  \\\n",
       "AlmadaNegreiros                   19                    0             1   \n",
       "CamiloCasteloBranco                0                   19             0   \n",
       "EcaDeQueiros                       0                    0            22   \n",
       "JoseRodriguesSantos                0                    0             0   \n",
       "JoseSaramago                       0                    0             0   \n",
       "LuisaMarquesSilva                  0                    0             0   \n",
       "\n",
       "                     JoseRodriguesSantos  JoseSaramago  LuisaMarquesSilva  \n",
       "AlmadaNegreiros                        0             0                  0  \n",
       "CamiloCasteloBranco                    0             0                  0  \n",
       "EcaDeQueiros                           0             0                  0  \n",
       "JoseRodriguesSantos                   28             1                  0  \n",
       "JoseSaramago                           0            26                  0  \n",
       "LuisaMarquesSilva                      0             0                 18  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm = metrics.confusion_matrix(y_test, prediction)\n",
    "pd.DataFrame(cm, index=df_train.loc[:,'author_name'].unique(), columns=df_train.loc[:,'author_name'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     precision    recall  f1-score   support\n",
      "\n",
      "    AlmadaNegreiros      1.000     0.950     0.974        20\n",
      "CamiloCasteloBranco      1.000     1.000     1.000        19\n",
      "       EcaDeQueiros      0.957     1.000     0.978        22\n",
      "JoseRodriguesSantos      1.000     0.966     0.982        29\n",
      "       JoseSaramago      0.963     1.000     0.981        26\n",
      "  LuisaMarquesSilva      1.000     1.000     1.000        18\n",
      "\n",
      "           accuracy                          0.985       134\n",
      "          macro avg      0.987     0.986     0.986       134\n",
      "       weighted avg      0.986     0.985     0.985       134\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_test, prediction, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['JoseSaramago', 'AlmadaNegreiros', 'LuisaMarquesSilva',\n",
       "       'EcaDeQueiros', 'CamiloCasteloBranco', 'JoseRodriguesSantos',\n",
       "       'JoseSaramago', 'AlmadaNegreiros', 'LuisaMarquesSilva',\n",
       "       'EcaDeQueiros', 'CamiloCasteloBranco', 'JoseRodriguesSantos'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(cbow_final_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_frequency = df_train.author_name.isin(['JoseRodriguesSantos','JoseSaramago','CamiloCasteloBranco'])\n",
    "medium_frequency = df_train.author_name.isin(['EcaDeQueiros'])\n",
    "df_train_balanced = pd.concat([df_train.loc[high_frequency,:].iloc[::5,:],\n",
    "                               df_train.loc[medium_frequency,:].iloc[::3,:],\n",
    "                               df_train.loc[~(high_frequency | medium_frequency),:]]).sort_index().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "JoseRodriguesSantos    463\n",
       "JoseSaramago           407\n",
       "CamiloCasteloBranco    310\n",
       "EcaDeQueiros           299\n",
       "AlmadaNegreiros         98\n",
       "LuisaMarquesSilva       90\n",
       "Name: author_name, dtype: int64"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_balanced.author_name.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_dev, y_train, y_dev = train_test_split(np.array(df_train_balanced.loc[:,'simple_clean']),\n",
    "                                                  np.array(df_train_balanced.loc[:,'author_name'].astype('category').cat.codes),\n",
    "                                                  test_size=0.2, \n",
    "                                                  random_state=42,\n",
    "                                                  stratify=df_train_balanced.loc[:,'author_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_train = pad_sequences(X_train, maxlen=500, truncating='post')\n",
    "y_train = to_categorical(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(130000, 128))\n",
    "model.add(LSTM(128, dropout=0.2))\n",
    "model.add(Dense(6, activation=\"softmax\"))\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/11\n",
      "1333/1333 [==============================] - ETA: 30s - loss: 1.7929 - accuracy: 0.203 - ETA: 27s - loss: 1.7913 - accuracy: 0.203 - ETA: 25s - loss: 1.7889 - accuracy: 0.208 - ETA: 23s - loss: 1.7873 - accuracy: 0.222 - ETA: 22s - loss: 1.7852 - accuracy: 0.240 - ETA: 20s - loss: 1.7830 - accuracy: 0.239 - ETA: 19s - loss: 1.7793 - accuracy: 0.241 - ETA: 17s - loss: 1.7773 - accuracy: 0.236 - ETA: 16s - loss: 1.7736 - accuracy: 0.241 - ETA: 14s - loss: 1.7712 - accuracy: 0.240 - ETA: 13s - loss: 1.7665 - accuracy: 0.240 - ETA: 12s - loss: 1.7524 - accuracy: 0.239 - ETA: 10s - loss: 1.7473 - accuracy: 0.239 - ETA: 9s - loss: 1.7338 - accuracy: 0.246 - ETA: 7s - loss: 1.7261 - accuracy: 0.24 - ETA: 6s - loss: 1.7211 - accuracy: 0.25 - ETA: 5s - loss: 1.7181 - accuracy: 0.25 - ETA: 3s - loss: 1.7128 - accuracy: 0.26 - ETA: 2s - loss: 1.7118 - accuracy: 0.26 - ETA: 1s - loss: 1.7070 - accuracy: 0.26 - 28s 21ms/step - loss: 1.7051 - accuracy: 0.2596\n",
      "Epoch 2/11\n",
      "1333/1333 [==============================] - ETA: 26s - loss: 1.5891 - accuracy: 0.281 - ETA: 24s - loss: 1.6127 - accuracy: 0.289 - ETA: 22s - loss: 1.6399 - accuracy: 0.270 - ETA: 22s - loss: 1.6199 - accuracy: 0.296 - ETA: 21s - loss: 1.6135 - accuracy: 0.306 - ETA: 19s - loss: 1.6085 - accuracy: 0.309 - ETA: 18s - loss: 1.6187 - accuracy: 0.301 - ETA: 17s - loss: 1.6215 - accuracy: 0.293 - ETA: 15s - loss: 1.6248 - accuracy: 0.295 - ETA: 14s - loss: 1.6210 - accuracy: 0.295 - ETA: 13s - loss: 1.6179 - accuracy: 0.295 - ETA: 11s - loss: 1.6189 - accuracy: 0.293 - ETA: 10s - loss: 1.6177 - accuracy: 0.292 - ETA: 9s - loss: 1.6152 - accuracy: 0.287 - ETA: 7s - loss: 1.6135 - accuracy: 0.28 - ETA: 6s - loss: 1.6061 - accuracy: 0.28 - ETA: 5s - loss: 1.6038 - accuracy: 0.27 - ETA: 3s - loss: 1.6083 - accuracy: 0.27 - ETA: 2s - loss: 1.6050 - accuracy: 0.27 - ETA: 1s - loss: 1.5986 - accuracy: 0.28 - 28s 21ms/step - loss: 1.5956 - accuracy: 0.2881\n",
      "Epoch 3/11\n",
      "1333/1333 [==============================] - ETA: 26s - loss: 1.4843 - accuracy: 0.312 - ETA: 25s - loss: 1.4763 - accuracy: 0.335 - ETA: 23s - loss: 1.4132 - accuracy: 0.411 - ETA: 22s - loss: 1.5147 - accuracy: 0.375 - ETA: 20s - loss: 1.5032 - accuracy: 0.390 - ETA: 19s - loss: 1.5000 - accuracy: 0.414 - ETA: 18s - loss: 1.4921 - accuracy: 0.457 - ETA: 16s - loss: 1.4980 - accuracy: 0.496 - ETA: 30s - loss: 1.4957 - accuracy: 0.526 - ETA: 26s - loss: 1.4990 - accuracy: 0.543 - ETA: 23s - loss: 1.5032 - accuracy: 0.562 - ETA: 20s - loss: 1.5041 - accuracy: 0.578 - ETA: 17s - loss: 1.5092 - accuracy: 0.581 - ETA: 14s - loss: 1.5078 - accuracy: 0.584 - ETA: 12s - loss: 1.5053 - accuracy: 0.589 - ETA: 10s - loss: 1.5025 - accuracy: 0.588 - ETA: 7s - loss: 1.5000 - accuracy: 0.585 - ETA: 5s - loss: 1.4946 - accuracy: 0.58 - ETA: 3s - loss: 1.4913 - accuracy: 0.56 - ETA: 1s - loss: 1.4845 - accuracy: 0.56 - 39s 29ms/step - loss: 1.4791 - accuracy: 0.5604\n",
      "Epoch 4/11\n",
      "1333/1333 [==============================] - ETA: 24s - loss: 1.2508 - accuracy: 0.562 - ETA: 24s - loss: 1.2795 - accuracy: 0.429 - ETA: 22s - loss: 1.2994 - accuracy: 0.421 - ETA: 21s - loss: 1.2857 - accuracy: 0.421 - ETA: 20s - loss: 1.2761 - accuracy: 0.440 - ETA: 19s - loss: 1.2629 - accuracy: 0.442 - ETA: 18s - loss: 1.2418 - accuracy: 0.462 - ETA: 16s - loss: 1.2208 - accuracy: 0.480 - ETA: 15s - loss: 1.2018 - accuracy: 0.503 - ETA: 14s - loss: 1.1915 - accuracy: 0.528 - ETA: 13s - loss: 1.1647 - accuracy: 0.539 - ETA: 11s - loss: 1.1438 - accuracy: 0.541 - ETA: 10s - loss: 1.1254 - accuracy: 0.542 - ETA: 9s - loss: 1.1098 - accuracy: 0.542 - ETA: 7s - loss: 1.0954 - accuracy: 0.53 - ETA: 6s - loss: 1.0897 - accuracy: 0.53 - ETA: 5s - loss: 1.0739 - accuracy: 0.54 - ETA: 4s - loss: 1.0632 - accuracy: 0.55 - ETA: 2s - loss: 1.0574 - accuracy: 0.56 - ETA: 1s - loss: 1.0522 - accuracy: 0.55 - 30s 23ms/step - loss: 1.0513 - accuracy: 0.5499\n",
      "Epoch 5/11\n",
      "1333/1333 [==============================] - ETA: 36s - loss: 0.9580 - accuracy: 0.625 - ETA: 35s - loss: 1.0186 - accuracy: 0.539 - ETA: 32s - loss: 0.9891 - accuracy: 0.593 - ETA: 30s - loss: 0.9513 - accuracy: 0.668 - ETA: 30s - loss: 0.9188 - accuracy: 0.721 - ETA: 28s - loss: 0.9028 - accuracy: 0.742 - ETA: 26s - loss: 0.8960 - accuracy: 0.738 - ETA: 24s - loss: 0.8780 - accuracy: 0.738 - ETA: 22s - loss: 0.8699 - accuracy: 0.730 - ETA: 20s - loss: 0.8579 - accuracy: 0.723 - ETA: 18s - loss: 0.8516 - accuracy: 0.708 - ETA: 16s - loss: 0.8403 - accuracy: 0.696 - ETA: 14s - loss: 0.8285 - accuracy: 0.692 - ETA: 12s - loss: 0.8169 - accuracy: 0.694 - ETA: 10s - loss: 0.8108 - accuracy: 0.692 - ETA: 8s - loss: 0.7988 - accuracy: 0.699 - ETA: 6s - loss: 0.7880 - accuracy: 0.70 - ETA: 5s - loss: 0.7822 - accuracy: 0.70 - ETA: 3s - loss: 0.7748 - accuracy: 0.70 - ETA: 1s - loss: 0.7683 - accuracy: 0.70 - 37s 28ms/step - loss: 0.7592 - accuracy: 0.7164\n",
      "Epoch 6/11\n",
      "1333/1333 [==============================] - ETA: 32s - loss: 0.4887 - accuracy: 0.906 - ETA: 30s - loss: 0.4939 - accuracy: 0.859 - ETA: 28s - loss: 0.4874 - accuracy: 0.849 - ETA: 26s - loss: 0.4780 - accuracy: 0.859 - ETA: 25s - loss: 0.4567 - accuracy: 0.868 - ETA: 23s - loss: 0.4511 - accuracy: 0.875 - ETA: 22s - loss: 0.4392 - accuracy: 0.877 - ETA: 21s - loss: 0.4288 - accuracy: 0.882 - ETA: 19s - loss: 0.4247 - accuracy: 0.885 - ETA: 17s - loss: 0.4215 - accuracy: 0.881 - ETA: 15s - loss: 0.4061 - accuracy: 0.884 - ETA: 13s - loss: 0.3942 - accuracy: 0.888 - ETA: 12s - loss: 0.3910 - accuracy: 0.883 - ETA: 10s - loss: 0.3811 - accuracy: 0.886 - ETA: 8s - loss: 0.3721 - accuracy: 0.888 - ETA: 7s - loss: 0.3705 - accuracy: 0.88 - ETA: 5s - loss: 0.3875 - accuracy: 0.87 - ETA: 4s - loss: 0.3911 - accuracy: 0.87 - ETA: 2s - loss: 0.3971 - accuracy: 0.87 - ETA: 1s - loss: 0.3974 - accuracy: 0.86 - 32s 24ms/step - loss: 0.3917 - accuracy: 0.8695\n",
      "Epoch 7/11\n",
      "1333/1333 [==============================] - ETA: 33s - loss: 0.2834 - accuracy: 0.921 - ETA: 35s - loss: 0.2749 - accuracy: 0.921 - ETA: 35s - loss: 0.2954 - accuracy: 0.916 - ETA: 31s - loss: 0.2832 - accuracy: 0.921 - ETA: 28s - loss: 0.2847 - accuracy: 0.918 - ETA: 26s - loss: 0.3012 - accuracy: 0.906 - ETA: 23s - loss: 0.3044 - accuracy: 0.899 - ETA: 21s - loss: 0.3014 - accuracy: 0.904 - ETA: 19s - loss: 0.2976 - accuracy: 0.904 - ETA: 17s - loss: 0.2933 - accuracy: 0.901 - ETA: 15s - loss: 0.2867 - accuracy: 0.904 - ETA: 13s - loss: 0.2832 - accuracy: 0.906 - ETA: 11s - loss: 0.2814 - accuracy: 0.907 - ETA: 10s - loss: 0.2763 - accuracy: 0.909 - ETA: 8s - loss: 0.2784 - accuracy: 0.912 - ETA: 7s - loss: 0.2733 - accuracy: 0.91 - ETA: 5s - loss: 0.2712 - accuracy: 0.91 - ETA: 4s - loss: 0.2667 - accuracy: 0.91 - ETA: 2s - loss: 0.2630 - accuracy: 0.91 - ETA: 1s - loss: 0.2593 - accuracy: 0.91 - 33s 25ms/step - loss: 0.2581 - accuracy: 0.9190\n",
      "Epoch 8/11\n",
      "1333/1333 [==============================] - ETA: 32s - loss: 0.1751 - accuracy: 0.968 - ETA: 29s - loss: 0.1417 - accuracy: 0.984 - ETA: 28s - loss: 0.1465 - accuracy: 0.974 - ETA: 26s - loss: 0.1434 - accuracy: 0.972 - ETA: 25s - loss: 0.1394 - accuracy: 0.978 - ETA: 24s - loss: 0.1368 - accuracy: 0.976 - ETA: 22s - loss: 0.1399 - accuracy: 0.971 - ETA: 21s - loss: 0.1335 - accuracy: 0.972 - ETA: 19s - loss: 0.1301 - accuracy: 0.972 - ETA: 18s - loss: 0.1305 - accuracy: 0.973 - ETA: 16s - loss: 0.1318 - accuracy: 0.973 - ETA: 15s - loss: 0.1312 - accuracy: 0.975 - ETA: 13s - loss: 0.1304 - accuracy: 0.976 - ETA: 11s - loss: 0.1278 - accuracy: 0.977 - ETA: 10s - loss: 0.1235 - accuracy: 0.979 - ETA: 8s - loss: 0.1216 - accuracy: 0.979 - ETA: 6s - loss: 0.1211 - accuracy: 0.98 - ETA: 4s - loss: 0.1209 - accuracy: 0.98 - ETA: 3s - loss: 0.1187 - accuracy: 0.98 - ETA: 1s - loss: 0.1155 - accuracy: 0.98 - 36s 27ms/step - loss: 0.1134 - accuracy: 0.9835\n",
      "Epoch 9/11\n",
      "1333/1333 [==============================] - ETA: 32s - loss: 0.0597 - accuracy: 1.000 - ETA: 31s - loss: 0.0679 - accuracy: 0.984 - ETA: 28s - loss: 0.0744 - accuracy: 0.974 - ETA: 27s - loss: 0.0686 - accuracy: 0.976 - ETA: 25s - loss: 0.0641 - accuracy: 0.978 - ETA: 23s - loss: 0.0598 - accuracy: 0.979 - ETA: 21s - loss: 0.0598 - accuracy: 0.977 - ETA: 20s - loss: 0.0560 - accuracy: 0.980 - ETA: 18s - loss: 0.0543 - accuracy: 0.982 - ETA: 16s - loss: 0.0559 - accuracy: 0.984 - ETA: 14s - loss: 0.0557 - accuracy: 0.985 - ETA: 13s - loss: 0.0536 - accuracy: 0.987 - ETA: 11s - loss: 0.0529 - accuracy: 0.988 - ETA: 10s - loss: 0.0516 - accuracy: 0.987 - ETA: 8s - loss: 0.0495 - accuracy: 0.988 - ETA: 7s - loss: 0.0487 - accuracy: 0.98 - ETA: 5s - loss: 0.0480 - accuracy: 0.98 - ETA: 4s - loss: 0.0470 - accuracy: 0.98 - ETA: 2s - loss: 0.0459 - accuracy: 0.99 - ETA: 1s - loss: 0.0446 - accuracy: 0.99 - 30s 23ms/step - loss: 0.0436 - accuracy: 0.9910\n",
      "Epoch 10/11\n",
      "1333/1333 [==============================] - ETA: 25s - loss: 0.0292 - accuracy: 1.000 - ETA: 24s - loss: 0.0217 - accuracy: 1.000 - ETA: 23s - loss: 0.0194 - accuracy: 1.000 - ETA: 21s - loss: 0.0196 - accuracy: 1.000 - ETA: 20s - loss: 0.0192 - accuracy: 1.000 - ETA: 19s - loss: 0.0182 - accuracy: 1.000 - ETA: 17s - loss: 0.0169 - accuracy: 1.000 - ETA: 16s - loss: 0.0172 - accuracy: 1.000 - ETA: 15s - loss: 0.0169 - accuracy: 1.000 - ETA: 14s - loss: 0.0187 - accuracy: 0.998 - ETA: 12s - loss: 0.0185 - accuracy: 0.998 - ETA: 11s - loss: 0.0187 - accuracy: 0.998 - ETA: 10s - loss: 0.0180 - accuracy: 0.998 - ETA: 8s - loss: 0.0187 - accuracy: 0.997 - ETA: 7s - loss: 0.0212 - accuracy: 0.99 - ETA: 6s - loss: 0.0225 - accuracy: 0.99 - ETA: 5s - loss: 0.0234 - accuracy: 0.99 - ETA: 3s - loss: 0.0228 - accuracy: 0.99 - ETA: 2s - loss: 0.0223 - accuracy: 0.99 - ETA: 1s - loss: 0.0223 - accuracy: 0.99 - 28s 21ms/step - loss: 0.0218 - accuracy: 0.9955\n",
      "Epoch 11/11\n",
      "1333/1333 [==============================] - ETA: 31s - loss: 0.0110 - accuracy: 1.000 - ETA: 31s - loss: 0.0155 - accuracy: 1.000 - ETA: 29s - loss: 0.0139 - accuracy: 1.000 - ETA: 27s - loss: 0.0150 - accuracy: 1.000 - ETA: 25s - loss: 0.0145 - accuracy: 1.000 - ETA: 24s - loss: 0.0135 - accuracy: 1.000 - ETA: 22s - loss: 0.0152 - accuracy: 1.000 - ETA: 21s - loss: 0.0150 - accuracy: 1.000 - ETA: 19s - loss: 0.0146 - accuracy: 1.000 - ETA: 17s - loss: 0.0156 - accuracy: 1.000 - ETA: 16s - loss: 0.0152 - accuracy: 1.000 - ETA: 14s - loss: 0.0149 - accuracy: 1.000 - ETA: 12s - loss: 0.0145 - accuracy: 1.000 - ETA: 11s - loss: 0.0142 - accuracy: 1.000 - ETA: 9s - loss: 0.0139 - accuracy: 1.000 - ETA: 8s - loss: 0.0135 - accuracy: 1.00 - ETA: 6s - loss: 0.0133 - accuracy: 1.00 - ETA: 4s - loss: 0.0132 - accuracy: 1.00 - ETA: 3s - loss: 0.0130 - accuracy: 1.00 - ETA: 1s - loss: 0.0126 - accuracy: 1.00 - 35s 26ms/step - loss: 0.0124 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1de8cca5f08>"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, batch_size=64, epochs=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dev = tokenizer.texts_to_sequences(X_dev)\n",
    "X_dev = pad_sequences(X_dev, maxlen=500, truncating='post')\n",
    "y_dev = to_categorical(y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "334/334 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 1s 4ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.0815331204208785, 0.7335329055786133]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_dev, y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.array(df_test.loc[:,'simple_clean'])\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "X_test = pad_sequences(X_test, maxlen=500, truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model.predict(X_test)\n",
    "y_pred = np.argmax(predicted, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_author = dict(zip(df_train.loc[:,'author_name'].astype('category').cat.codes, df_train.loc[:,'author_name']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['JoseSaramago', 'CamiloCasteloBranco', 'JoseSaramago',\n",
       "       'EcaDeQueiros', 'CamiloCasteloBranco', 'JoseRodriguesSantos',\n",
       "       'JoseSaramago', 'CamiloCasteloBranco', 'JoseSaramago',\n",
       "       'EcaDeQueiros', 'CamiloCasteloBranco', 'JoseRodriguesSantos'],\n",
       "      dtype='<U19')"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([dict_author[i] for i in y_pred])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_frequency = df_train.author_name.isin(['JoseRodriguesSantos','JoseSaramago','CamiloCasteloBranco'])\n",
    "medium_frequency = df_train.author_name.isin(['EcaDeQueiros'])\n",
    "df_train_balanced = pd.concat([df_train.loc[high_frequency,:].iloc[::5,:],\n",
    "                               df_train.loc[medium_frequency,:].iloc[::3,:],\n",
    "                               df_train.loc[~(high_frequency | medium_frequency),:]]).sort_index().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "JoseRodriguesSantos    463\n",
       "JoseSaramago           407\n",
       "CamiloCasteloBranco    310\n",
       "EcaDeQueiros           299\n",
       "AlmadaNegreiros         98\n",
       "LuisaMarquesSilva       90\n",
       "Name: author_name, dtype: int64"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_balanced.author_name.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_dev, y_train, y_dev = train_test_split(np.array(df_train_balanced.loc[:,'simple_clean']),\n",
    "                                                  np.array(df_train_balanced.loc[:,'author_name'].astype('category').cat.codes),\n",
    "                                                  test_size=0.2, \n",
    "                                                  random_state=42,\n",
    "                                                  stratify=df_train_balanced.loc[:,'author_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_train = pad_sequences(X_train, maxlen=500, truncating='post')\n",
    "y_train = to_categorical(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dev = tokenizer.texts_to_sequences(X_dev)\n",
    "X_dev = pad_sequences(X_dev, maxlen=500, truncating='post')\n",
    "y_dev = to_categorical(y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(80000, 128))\n",
    "model.add(Conv1D(filters=256, kernel_size=13, activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(32,activation='relu'))\n",
    "model.add(Dense(6, activation='softmax') )\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1333 samples, validate on 334 samples\n",
      "Epoch 1/5\n",
      "1333/1333 [==============================] - ETA: 14s - loss: 1.8238 - accuracy: 0.109 - ETA: 11s - loss: 1.8217 - accuracy: 0.125 - ETA: 10s - loss: 1.8210 - accuracy: 0.130 - ETA: 10s - loss: 1.8061 - accuracy: 0.164 - ETA: 9s - loss: 1.7959 - accuracy: 0.175 - ETA: 8s - loss: 1.7800 - accuracy: 0.20 - ETA: 8s - loss: 1.7762 - accuracy: 0.20 - ETA: 7s - loss: 1.7732 - accuracy: 0.20 - ETA: 6s - loss: 1.7687 - accuracy: 0.20 - ETA: 6s - loss: 1.7623 - accuracy: 0.21 - ETA: 5s - loss: 1.7570 - accuracy: 0.21 - ETA: 5s - loss: 1.7541 - accuracy: 0.22 - ETA: 4s - loss: 1.7467 - accuracy: 0.22 - ETA: 3s - loss: 1.7424 - accuracy: 0.23 - ETA: 3s - loss: 1.7418 - accuracy: 0.23 - ETA: 2s - loss: 1.7414 - accuracy: 0.23 - ETA: 2s - loss: 1.7380 - accuracy: 0.23 - ETA: 1s - loss: 1.7351 - accuracy: 0.24 - ETA: 1s - loss: 1.7328 - accuracy: 0.24 - ETA: 0s - loss: 1.7297 - accuracy: 0.24 - 13s 10ms/step - loss: 1.7277 - accuracy: 0.2476 - val_loss: 1.6492 - val_accuracy: 0.2784\n",
      "Epoch 2/5\n",
      "1333/1333 [==============================] - ETA: 11s - loss: 1.5953 - accuracy: 0.312 - ETA: 11s - loss: 1.5528 - accuracy: 0.328 - ETA: 10s - loss: 1.5855 - accuracy: 0.296 - ETA: 10s - loss: 1.5989 - accuracy: 0.300 - ETA: 9s - loss: 1.6134 - accuracy: 0.293 - ETA: 9s - loss: 1.6135 - accuracy: 0.30 - ETA: 8s - loss: 1.5998 - accuracy: 0.31 - ETA: 7s - loss: 1.6017 - accuracy: 0.30 - ETA: 7s - loss: 1.6043 - accuracy: 0.31 - ETA: 6s - loss: 1.6014 - accuracy: 0.32 - ETA: 5s - loss: 1.6053 - accuracy: 0.32 - ETA: 5s - loss: 1.6024 - accuracy: 0.33 - ETA: 4s - loss: 1.5948 - accuracy: 0.33 - ETA: 4s - loss: 1.5966 - accuracy: 0.34 - ETA: 3s - loss: 1.5912 - accuracy: 0.35 - ETA: 2s - loss: 1.5937 - accuracy: 0.35 - ETA: 2s - loss: 1.5866 - accuracy: 0.35 - ETA: 1s - loss: 1.5903 - accuracy: 0.34 - ETA: 1s - loss: 1.5840 - accuracy: 0.35 - ETA: 0s - loss: 1.5813 - accuracy: 0.35 - 13s 10ms/step - loss: 1.5807 - accuracy: 0.3533 - val_loss: 1.6145 - val_accuracy: 0.2784\n",
      "Epoch 3/5\n",
      "1333/1333 [==============================] - ETA: 11s - loss: 1.4688 - accuracy: 0.531 - ETA: 11s - loss: 1.5206 - accuracy: 0.445 - ETA: 10s - loss: 1.4916 - accuracy: 0.468 - ETA: 10s - loss: 1.5056 - accuracy: 0.433 - ETA: 9s - loss: 1.4783 - accuracy: 0.428 - ETA: 8s - loss: 1.4694 - accuracy: 0.43 - ETA: 8s - loss: 1.4792 - accuracy: 0.42 - ETA: 7s - loss: 1.4818 - accuracy: 0.42 - ETA: 7s - loss: 1.4866 - accuracy: 0.42 - ETA: 6s - loss: 1.4880 - accuracy: 0.43 - ETA: 6s - loss: 1.4834 - accuracy: 0.43 - ETA: 5s - loss: 1.4819 - accuracy: 0.44 - ETA: 4s - loss: 1.4878 - accuracy: 0.45 - ETA: 4s - loss: 1.4912 - accuracy: 0.45 - ETA: 3s - loss: 1.4868 - accuracy: 0.45 - ETA: 2s - loss: 1.4856 - accuracy: 0.45 - ETA: 2s - loss: 1.4821 - accuracy: 0.46 - ETA: 1s - loss: 1.4803 - accuracy: 0.46 - ETA: 1s - loss: 1.4765 - accuracy: 0.46 - ETA: 0s - loss: 1.4742 - accuracy: 0.46 - 14s 10ms/step - loss: 1.4720 - accuracy: 0.4719 - val_loss: 1.5608 - val_accuracy: 0.3024\n",
      "Epoch 4/5\n",
      "1333/1333 [==============================] - ETA: 11s - loss: 1.3193 - accuracy: 0.609 - ETA: 11s - loss: 1.3094 - accuracy: 0.625 - ETA: 10s - loss: 1.3211 - accuracy: 0.604 - ETA: 10s - loss: 1.3067 - accuracy: 0.621 - ETA: 9s - loss: 1.3114 - accuracy: 0.637 - ETA: 8s - loss: 1.3121 - accuracy: 0.64 - ETA: 8s - loss: 1.2987 - accuracy: 0.65 - ETA: 7s - loss: 1.3091 - accuracy: 0.65 - ETA: 7s - loss: 1.3011 - accuracy: 0.65 - ETA: 6s - loss: 1.3109 - accuracy: 0.65 - ETA: 5s - loss: 1.3062 - accuracy: 0.65 - ETA: 5s - loss: 1.3048 - accuracy: 0.66 - ETA: 4s - loss: 1.2928 - accuracy: 0.67 - ETA: 4s - loss: 1.2833 - accuracy: 0.68 - ETA: 3s - loss: 1.2690 - accuracy: 0.69 - ETA: 2s - loss: 1.2563 - accuracy: 0.70 - ETA: 2s - loss: 1.2545 - accuracy: 0.70 - ETA: 1s - loss: 1.2493 - accuracy: 0.71 - ETA: 1s - loss: 1.2469 - accuracy: 0.70 - ETA: 0s - loss: 1.2466 - accuracy: 0.70 - 13s 10ms/step - loss: 1.2435 - accuracy: 0.7104 - val_loss: 1.4541 - val_accuracy: 0.4072\n",
      "Epoch 5/5\n",
      "1333/1333 [==============================] - ETA: 11s - loss: 1.0555 - accuracy: 0.781 - ETA: 11s - loss: 1.0670 - accuracy: 0.757 - ETA: 10s - loss: 0.9659 - accuracy: 0.807 - ETA: 10s - loss: 0.9700 - accuracy: 0.804 - ETA: 9s - loss: 0.9736 - accuracy: 0.793 - ETA: 8s - loss: 0.9531 - accuracy: 0.80 - ETA: 8s - loss: 0.9483 - accuracy: 0.80 - ETA: 7s - loss: 0.9348 - accuracy: 0.81 - ETA: 7s - loss: 0.9356 - accuracy: 0.80 - ETA: 6s - loss: 0.9190 - accuracy: 0.81 - ETA: 5s - loss: 0.9015 - accuracy: 0.81 - ETA: 5s - loss: 0.8872 - accuracy: 0.82 - ETA: 4s - loss: 0.8772 - accuracy: 0.82 - ETA: 4s - loss: 0.8715 - accuracy: 0.82 - ETA: 3s - loss: 0.8596 - accuracy: 0.82 - ETA: 2s - loss: 0.8543 - accuracy: 0.82 - ETA: 2s - loss: 0.8466 - accuracy: 0.83 - ETA: 1s - loss: 0.8363 - accuracy: 0.83 - ETA: 1s - loss: 0.8302 - accuracy: 0.83 - ETA: 0s - loss: 0.8315 - accuracy: 0.83 - 14s 10ms/step - loss: 0.8298 - accuracy: 0.8327 - val_loss: 1.0513 - val_accuracy: 0.7605\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1de8b361b88>"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, validation_data=(X_dev,y_dev), batch_size=64, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "334/334 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 1s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.0512918226733179, 0.7604790329933167]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_dev, y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.array(df_test.loc[:,'simple_clean'])\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "X_test = pad_sequences(X_test, maxlen=500, truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model.predict(X_test)\n",
    "y_pred = np.argmax(predicted, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_author = dict(zip(df_train.loc[:,'author_name'].astype('category').cat.codes, df_train.loc[:,'author_name']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['JoseSaramago', 'CamiloCasteloBranco', 'JoseSaramago',\n",
       "       'EcaDeQueiros', 'CamiloCasteloBranco', 'JoseRodriguesSantos',\n",
       "       'JoseSaramago', 'CamiloCasteloBranco', 'JoseRodriguesSantos',\n",
       "       'EcaDeQueiros', 'CamiloCasteloBranco', 'JoseRodriguesSantos'],\n",
       "      dtype='<U19')"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([dict_author[i] for i in y_pred])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
